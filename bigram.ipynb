{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c936ba66-9ff2-487d-8c00-cdb6d7207582",
   "metadata": {},
   "source": [
    "\n",
    "# Making a Simple Bigram model with Romeo and Juliet Book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d305fd9e-7de7-4d1e-a52c-7ffddd41f250",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Mac\\Home\\Documents\\YungtingGPT\\yuGPT\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Mac\\Home\\Documents\\YungtingGPT\\yuGPT\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\Mac\\Home\\Documents\\YungtingGPT\\yuGPT\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d95fde-e3cf-4088-840b-9487310ce515",
   "metadata": {},
   "source": [
    "\n",
    "## 1) read the text file with data and store it in a variable\n",
    "\n",
    "    the contents of the book was stored in a `data.txt file` and all it['s content is read and copied to a variable named text.\n",
    "    the length and the first 300 characters was printed to test if it was working and once confirmed moved on the second part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f2d4b2-192e-4ce0-8c80-71ae53f9da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()  \n",
    "print(len(text))\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48074416-f5f4-4c5e-b090-5ddd0c0316bf",
   "metadata": {},
   "source": [
    "\n",
    "## 2) make a sorted set of all the characters in the dataset\n",
    "\n",
    "    a set of all the unique characters in the data was taken using the `set()` method and the result was passed into the `sorted()` method and copied   to the `chars` variable, The result is printed for testing and visualisation purposes. This helps us have a vocabulary of sorts to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a664f27-da77-4582-8a2d-ef9096131346",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a86e1-2305-40ea-a9ac-8c0155d15abc",
   "metadata": {},
   "source": [
    "\n",
    " ### why do the above ? \n",
    "\n",
    "  this lets us work with tokenizers which comes with encoders and decoders, encoders can help us convert all the 71 characters to numbers. we will be making a character level tokenizer below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbca93b-33c7-4c03-9789-ad1d876ae256",
   "metadata": {},
   "source": [
    "\n",
    "## 3) make a basic character-level tokenizer\n",
    "\n",
    "\n",
    "In order to make the character level tokenizer two dictionaries are first made. **'string_to_integer = { ch:i for i,ch in enumerate(chars) }'** enumerate over each character in the array and assign it an unique number making the first dictionary and reverse for integer to string making the second dictionary .\n",
    "\n",
    "**e.g. :** if  'chars = ['a', 'b', 'c']' then these will be the values of the two dictionaries :\n",
    "string_to_integer = {'a': 0, 'b': 1, 'c': 2}\n",
    "integer_to_string = {0: 'a', 1: 'b', 2: 'c'}\n",
    "\n",
    "the encoder lambda function takes a string s and converts it to a list of integers. For each character c in the string s, it looks up its integer value using the string_to_integer dictionary. The result is a list of integers corresponding to each character.\n",
    "\n",
    "the decoder lambda function takes a list of integers l and converts it back to a string. For each integer i in the list l, it looks up the corresponding character using the integer_to_string dictionary. It then joins these characters into a single string.\n",
    "\n",
    "**another e.g:** \n",
    "\n",
    "encoder('abc')  # e.g. Output: [0, 1, 2]\n",
    "\n",
    "decoder([0, 1, 2])  # e.g. Output: 'abc'\n",
    "\n",
    "these two methods combied helps have the functionality of a basic character-level tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926ad32-05f5-4e91-8abe-4a17530f5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_integer = { ch:i for i,ch in enumerate(chars) }\n",
    "integer_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encoder = lambda s: [string_to_integer[c] for c in s]\n",
    "decoder = lambda l: ''.join([integer_to_string[i] for i in l])\n",
    "\n",
    "encoded_yungting = encoder('yungting')\n",
    "print(encoded_yungting)\n",
    "decoded_yungting = decoder(encoded_yungting)\n",
    "print(decoded_yungting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a9884-377a-4aa5-88f0-6de60585f8f5",
   "metadata": {},
   "source": [
    "\n",
    "### <ins>More info on tokenizers</ins>\n",
    "\n",
    " ### Types of tokenizers \n",
    "\n",
    " #### 1) Character-level tokenizer\n",
    "\n",
    "- Small Vocabulary but large amount of chcaracters to work with.For example, for the romeo and julirt data we are using we have 142466 characters with a vocabulary of 71 unique characters. There are more characters to encode and decode.\n",
    "\n",
    "\n",
    " #### 2) word-level tokenizer \n",
    "\n",
    "- Very large Vocabulary but smaller amount of words to work with.The vocabulary can get very big especially if there is multiple languages.Less words to encode and decode but very large dictionary\n",
    "\n",
    "#### 3) subword-tokenizer\n",
    "\n",
    "- Somewhere inbetween character-level tokenizer and word-level tokenizer\n",
    "\n",
    "\n",
    "##### it is very important to be efficient with data when working with language models\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85653398-ba7d-4885-ad28-0abdadb612db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb1e69a-096b-4d47-85d0-be33d5c2a597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yungting-gpt",
   "language": "python",
   "name": "yugpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
