{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c936ba66-9ff2-487d-8c00-cdb6d7207582",
   "metadata": {},
   "source": [
    "\n",
    "# Making a Simple Bigram model with Romeo and Juliet Book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d305fd9e-7de7-4d1e-a52c-7ffddd41f250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d95fde-e3cf-4088-840b-9487310ce515",
   "metadata": {},
   "source": [
    "\n",
    "## 1) read the text file with data and store it in a variable\n",
    "\n",
    "    the contents of the book was stored in a `data.txt file` and all it['s content is read and copied to a variable named text.\n",
    "    the length and the first 300 characters was printed to test if it was working and once confirmed moved on the second part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f2d4b2-192e-4ce0-8c80-71ae53f9da15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142466\n",
      "﻿THE TRAGEDY OF ROMEO AND JULIET\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "THE PROLOGUE.\n",
      "\n",
      "ACT I\n",
      "Scene I. A public place.\n",
      "Scene II. A Street.\n",
      "Scene III. Room in Capulet’s House.\n",
      "Scene IV. A Street.\n",
      "Scene V. A Hall in Capulet’s House.\n",
      "\n",
      "ACT II\n",
      "CHORUS.\n",
      "Scene I. An open place adjoining Capulet’s Garden.\n",
      "Scen\n"
     ]
    }
   ],
   "source": [
    "with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()  \n",
    "print(len(text))\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48074416-f5f4-4c5e-b090-5ddd0c0316bf",
   "metadata": {},
   "source": [
    "\n",
    "## 2) make a sorted set of all the characters in the dataset\n",
    "\n",
    "    a set of all the unique characters in the data was taken using the `set()` method and the result was passed into the `sorted()` method and copied   to the `chars` variable, The result is printed for testing and visualisation purposes. This helps us have a vocabulary of sorts to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc2dda9-c34e-4a13-b1d6-54fbc1c83aa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '&', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', '—', '‘', '’', '“', '”', '\\ufeff']\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "print(len(chars))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a86e1-2305-40ea-a9ac-8c0155d15abc",
   "metadata": {},
   "source": [
    "\n",
    " ### why do the above ? \n",
    "\n",
    "  this lets us work with tokenizers which comes with encoders and decoders, encoders can help us convert all the 71 characters to numbers. we will be making a character level tokenizer below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbca93b-33c7-4c03-9789-ad1d876ae256",
   "metadata": {},
   "source": [
    "\n",
    "## 3) make a basic character-level tokenizer\n",
    "\n",
    "\n",
    "In order to make the character level tokenizer two dictionaries are first made. **'string_to_integer = { ch:i for i,ch in enumerate(chars) }'** enumerate over each character in the array and assign it an unique number making the first dictionary and reverse for integer to string making the second dictionary .\n",
    "\n",
    "**e.g. :** if  'chars = ['a', 'b', 'c']' then these will be the values of the two dictionaries :\n",
    "string_to_integer = {'a': 0, 'b': 1, 'c': 2}\n",
    "integer_to_string = {0: 'a', 1: 'b', 2: 'c'}\n",
    "\n",
    "the encoder lambda function takes a string s and converts it to a list of integers. For each character c in the string s, it looks up its integer value using the string_to_integer dictionary. The result is a list of integers corresponding to each character.\n",
    "\n",
    "the decoder lambda function takes a list of integers l and converts it back to a string. For each integer i in the list l, it looks up the corresponding character using the integer_to_string dictionary. It then joins these characters into a single string.\n",
    "\n",
    "**another e.g:** \n",
    "\n",
    "encoder('abc')  # e.g. Output: [0, 1, 2]\n",
    "\n",
    "decoder([0, 1, 2])  # e.g. Output: 'abc'\n",
    "\n",
    "these two methods combied helps have the functionality of a basic character-level tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b926ad32-05f5-4e91-8abe-4a17530f5c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 58, 51, 44, 57, 46, 51, 44]\n",
      "yungting\n"
     ]
    }
   ],
   "source": [
    "string_to_integer = { ch:i for i,ch in enumerate(chars) }\n",
    "integer_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encoder = lambda s: [string_to_integer[c] for c in s]\n",
    "decoder = lambda l: ''.join([integer_to_string[i] for i in l])\n",
    "\n",
    "encoded_yungting = encoder('yungting')\n",
    "print(encoded_yungting)\n",
    "decoded_yungting = decoder(encoded_yungting)\n",
    "print(decoded_yungting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a9884-377a-4aa5-88f0-6de60585f8f5",
   "metadata": {},
   "source": [
    "\n",
    "### <ins>More info on tokenizers</ins>\n",
    "\n",
    " ### Types of tokenizers \n",
    "\n",
    " #### 1) Character-level tokenizer\n",
    "\n",
    "- Small Vocabulary but large amount of chcaracters to work with.For example, for the romeo and julirt data we are using we have 142466 characters with a vocabulary of 71 unique characters. There are more characters to encode and decode.\n",
    "\n",
    "\n",
    " #### 2) word-level tokenizer \n",
    "\n",
    "- Very large Vocabulary but smaller amount of words to work with.The vocabulary can get very big especially if there is multiple languages.Less words to encode and decode but very large dictionary\n",
    "\n",
    "#### 3) subword-tokenizer\n",
    "\n",
    "- Somewhere inbetween character-level tokenizer and word-level tokenizer\n",
    "\n",
    "\n",
    "##### it is very important to be efficient with data when working with language models\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79cdf9-dc33-4e84-a038-9555c2f018d9",
   "metadata": {},
   "source": [
    "## 4) creating and working with tensors \n",
    "\n",
    "#### 1) What are tensors\n",
    "- tensors are the main data structure we will be working with , pytorch does most of the math for it for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb1e69a-096b-4d47-85d0-be33d5c2a597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3,4)\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8d3e6-1bcb-4e82-beed-4658bbe2a73c",
   "metadata": {},
   "source": [
    "- as seen above x is of the type Tensor and it is 3 rows and 4 columns and is empty\n",
    "- 1 dimensional tensor is usally called a vector and is reffered to as such\n",
    "- 2 dimensional tensor is usually called a matrix and referred to as such\n",
    "- 3 dimensional and above tensors are usually just referred to as tensors\n",
    "- as per definitions above we can refer to x as a matrix as it is 2 dimensional\n",
    "- x i populated with 32 bit floats by defaults , if you see random values it is just the last values from memory\n",
    "- We use tensors over arrays and stuff as it is easier for pytorch to work with it especially when reshaping, changing dimensionality,multiplying, doing dot products, and so on\n",
    "\n",
    "#### 2) Working with tensors and our enncoder we made \n",
    "\n",
    "\n",
    "- we will use our encoder to encode the entire 'Romeo and Juliet' novel into numbers then we will use torch tensor to make a tensor with the numbers returned from the encoder we made , while making the tensor we can manually set the data type to be sure that the tensor we are making is gonna use the mentioned data type of long\n",
    "- the first 120 characters will be printed out as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bdd9b5f-3acd-461d-9bbb-6f42ccbf4379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([70, 29, 17, 14,  1, 29, 27, 10, 16, 14, 13, 33,  1, 24, 15,  1, 27, 24,\n",
      "        22, 14, 24,  1, 10, 23, 13,  1, 19, 30, 21, 18, 14, 29,  0,  0, 39, 62,\n",
      "         1, 32, 46, 49, 49, 46, 38, 50,  1, 28, 45, 38, 48, 42, 56, 53, 42, 38,\n",
      "        55, 42,  0,  0,  0,  0,  0, 12, 52, 51, 57, 42, 51, 57, 56,  0,  0, 29,\n",
      "        17, 14,  1, 25, 27, 24, 21, 24, 16, 30, 14,  6,  0,  0, 10, 12, 29,  1,\n",
      "        18,  0, 28, 40, 42, 51, 42,  1, 18,  6,  1, 10,  1, 53, 58, 39, 49, 46,\n",
      "        40,  1, 53, 49, 38, 40, 42,  6,  0, 28, 40, 42])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(data[:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df6127-e962-484e-b9d9-32aa35fc1229",
   "metadata": {},
   "source": [
    "## 5) Creating training and Validation splits\n",
    "\n",
    "-  #### i) why do we need to split our data into training and validation splits ?\n",
    "     - we split it this way as if we just train the model on 100% of the text after some iterations the model will just memorise the text and would just produce the exact copy of our traiiong text every time, This is not the purpose of training a model as we are trying to train a language model that will complete the text we provide in a similar manner/style to our text from the training data.\n",
    "     - In order to achieve the goal mentioned above we might split our text so that 80% is training data and the rest 20% is validation data.\n",
    "     - e.g. : let's say we are training a model on data that goes sonething like this:\n",
    "         - Yung Ting after infiltrating a maximum security facility in atlantis has decided to make a copy of a top secret recipe that also contains the manfucating process of the viral atlanade drink with the unique...Yung Ting has achieved her goal and atlantis has been added to her list of evergrowing victims.\n",
    "         - let's say the above text is 100 lines of equal tokens per line for the sake of the example. We would use the first 80 lines info as training data and the last 20 lines as validation data making a very simple training and validation split.\n",
    "-  #### ii) how does bigrams come into play in what we are doing here in this simple and basic model ?\n",
    "    - it is called a bigram model as the simple model that is being made here today predicts only based on bigrams, i.e for e.g let's say the model is predicting the word 'yungting' it goes like this (very simple character level example) :\n",
    "        - start -> y\n",
    "        - y -> u\n",
    "        - u -> n\n",
    "        - n -> g\n",
    "        - g -> t\n",
    "        - t -> i\n",
    "        - i -> n\n",
    "        - n -> g\n",
    "    - working in bigrams and would only take the preceding character into account when predicting the next (for now)\n",
    "-  #### iii) how do we train a bigram model to achieve the goals above ? and how are we gonna use an artifical neural network to achieve it\n",
    "    - we would achieve the goals  above by using something called block sizes, we will make **predictions** and **targets** out of them\n",
    "    - e.g. block_size = 7\n",
    "        - context . . . [ 19 , 6 , 25 , 4 , 59 , 20 , 18 ] 21 . . . (predictions)\n",
    "        - context . . . 19 [ 6 , 25 , 4 , 59 , 20 , 18 , 21 ] . . . (targets)\n",
    "        - [] is a tensor in the lines above\n",
    "        - first tensor in python can be [:7] taking the first 7 characters, in the target it can be [1:block_size + 1] making an offset of 1\n",
    "        - we will figure out how far away the predictions are from the target and optimise it so that it is better\n",
    "        - allright now we can do what we just described but in code below starting with the training and validation splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eb40a8f-a4b5-4fab-a4ee-ebc6e1577068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(in characters) when context is ﻿ target value is :  T\n",
      "(In tensors) when context is tensor([70]) target value is :  tensor(29)\n",
      "(in characters) when context is ﻿T target value is :  H\n",
      "(In tensors) when context is tensor([70, 29]) target value is :  tensor(17)\n",
      "(in characters) when context is ﻿TH target value is :  E\n",
      "(In tensors) when context is tensor([70, 29, 17]) target value is :  tensor(14)\n",
      "(in characters) when context is ﻿THE target value is :   \n",
      "(In tensors) when context is tensor([70, 29, 17, 14]) target value is :  tensor(1)\n",
      "(in characters) when context is ﻿THE  target value is :  T\n",
      "(In tensors) when context is tensor([70, 29, 17, 14,  1]) target value is :  tensor(29)\n",
      "(in characters) when context is ﻿THE T target value is :  R\n",
      "(In tensors) when context is tensor([70, 29, 17, 14,  1, 29]) target value is :  tensor(27)\n",
      "(in characters) when context is ﻿THE TR target value is :  A\n",
      "(In tensors) when context is tensor([70, 29, 17, 14,  1, 29, 27]) target value is :  tensor(10)\n"
     ]
    }
   ],
   "source": [
    "s = int(0.7*len(data))\n",
    "training_data = data[:s]\n",
    "validation_data = data[:s]\n",
    "\n",
    "block_size = 7\n",
    "\n",
    "a = training_data[:block_size]\n",
    "b = training_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = a[:t+1]\n",
    "    target = b[t]\n",
    "    print(\"(in characters) when context is\", decoder(context.cpu().detach().numpy()), \"target value is : \",  decoder([target.cpu().detach().numpy().max().item()]) )\n",
    "    print(\"(In tensors) when context is\", context, \"target value is : \", target )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a86be1-f748-49e1-bd4e-67800c173726",
   "metadata": {},
   "source": [
    "## 6) Defining the get_batch function\n",
    "- #### i) what are batches ? \n",
    "    -  what we have done so far is sequential which is not scalable and will be slow, instead what we can do is we can use our gpu and use a lot more cores at onces in parallel using batches. Using our example above we would just have multiple blocks and push it in our gpu for calculation in parallel.\n",
    "    -  without batches performance will be poor\n",
    "- #### ii) what are batch sizes ?   \n",
    "    -  Our batch size is how many blocks we are processing in parallel, and block size is the length of each individual block\n",
    "- #### iii) how do we use batches?\n",
    "    - using batches is very simple with pytorch is very simple and is shown below **(gpu required)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c787bb00-f791-447b-8b7a-99259128f302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "inputs: \n",
      "tensor([[16, 27, 14, 16, 24, 27, 33],\n",
      "        [45,  4,  1, 57, 52, 52,  1],\n",
      "        [42, 55, 50, 56,  0, 23, 52],\n",
      "        [29, 45, 42,  1, 40, 46, 57],\n",
      "        [45, 52, 58,  1, 60, 52, 58],\n",
      "        [42,  1, 43, 38, 50, 46, 49]], device='cuda:0')\n",
      "targets: \n",
      "tensor([[27, 14, 16, 24, 27, 33,  6],\n",
      "        [ 4,  1, 57, 52, 52,  1, 58],\n",
      "        [55, 50, 56,  0, 23, 52, 55],\n",
      "        [45, 42,  1, 40, 46, 57, 46],\n",
      "        [52, 58,  1, 60, 52, 58, 49],\n",
      "        [ 1, 43, 38, 50, 46, 49, 62]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "batch_size = 6\n",
    "\n",
    "def get_batch(split):\n",
    "    data = training_data if split == 'train' else validation_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs: ')\n",
    "print(x)\n",
    "print('targets: ')\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d85b67-01c9-4b19-b291-152551a35200",
   "metadata": {},
   "source": [
    "\n",
    "- #### iv) more on the get_batch function defined above\n",
    "  - the if statement is there to decide wether to create batch for training data or validation data\n",
    "    - training data as mentioned before is the \"known content\" to train with and validation data is \"unkown\" content we will try and predict\n",
    "    - ix takes a random integer between 0 and length of data - block_size (we get random indices in the text we can start geenrating from)\n",
    "    - we use torch.stack to stack x and y in batches, y is offset from x by 1\n",
    "    - ```x,y = x.to(device), y.to(device)```is used to load the tensors into the gpu (as shown by device = 'cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e644043-1263-4b77-9dfc-99e75b6c704d",
   "metadata": {},
   "source": [
    "## 7) initializing a neural network\n",
    "\n",
    "- #### i) what is gradient descent ?\n",
    "\n",
    "    - Gradient descent helps us minimise the loss by adjusting model parameters itteratively\n",
    "    - how do we find if we are reducing the loss? we can take the derivative at the current point and move it in a direction (let's the slope is increasing in a negative direction so we can keep adjusting the slope in favor of this direction and basically descend with the gradient)\n",
    "    - gradient descents are oprimizers\n",
    "      \n",
    "- #### ii) how is loss calculated ?\n",
    "\n",
    "    - as for how the loss is calculated , let's say we have 80 possible characters with a chance of 1/80 for the prediction we will do -ln(1/80) which around ~4.382 (less than 2% chance)\n",
    "\n",
    "- #### iii) what are optimizers ?\n",
    "\n",
    "    - There are many optimisers, we will use AdamW which combines ideas from momentum and it uses a moving average of both the gradient and it's square parameter to adapt the learning rate of each parameter from Adam and adds weight decay to the adam algorithm (it generalises the parameters more)\n",
    "    - weight decay generalises the perfomance insrtead of having high or low level performance, weight significance will shrink as graph flattens out.\n",
    "    - lets say some weights give insanely high or low performance weight decay , will decay the performance to normalise it.\n",
    "\n",
    "- #### iv) what is Learning Rate ?\n",
    "\n",
    "    -  A hyperparameter that determines the size of the steps taken during optimization.\n",
    "    -  Too high can overshoot minima\n",
    "    -  Too low leads to slow convergence.\n",
    "    -  we wanna have a small learning rate but not too small.\n",
    "\n",
    "- #### v) what is an Embedding Table ?\n",
    "\n",
    "    - A lookup table representing how each token (character) relates to every other token.\n",
    "    - Typically, it’s a matrix of size ```vocab_size x vocab_size```\n",
    "    - shows a probably distribution of what character comes next given one character\n",
    "    - visualisation :\n",
    "    <div>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*RUaIk8LE1vty-J0blNE_eA.png\" width=\"400\"/>\n",
    "    </div>\n",
    "\n",
    "      \n",
    "- #### vi) what a custom forward pass ?\n",
    "\n",
    "    - we do a custom implementation as it is a very good practice when we have specific use cases we know what exactly is going on behind the scenes in the model, what transformations we are doing, how we are storing it and a lot of information that will help us debug.\n",
    "    - we can see how the input is tranformed step by step in each layer to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84757661-7459-4812-bd55-25a083f1ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size , vocab_size)\n",
    "\n",
    "    def forward_pass(self, index, targets):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718819ff-f5c7-4b23-bd13-943ee42acae3",
   "metadata": {},
   "source": [
    "## What are logits ?\n",
    "\n",
    "- let's normalise [2,4,6]\n",
    "    - total = 2 + 4 + 6 = 12\n",
    "    -  2 / 12 = 0.167 (16.7% chance)\n",
    "    -  4 / 12 = 0.333 (33.3% chance) \n",
    "    -  6 / 12 = 0.5 (50% chance)\n",
    "    - so a logit is basically [0.167, 0.333, 0.5]\n",
    "    - so let's say first represents ab , second ac , third ad , we know ad is most likely gonna happen next as there is a 50% chance\n",
    "    - basically like a probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131a8ba-bb74-48a1-83d8-43c4bc950ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yungting-gpt",
   "language": "python",
   "name": "yugpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
