{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c936ba66-9ff2-487d-8c00-cdb6d7207582",
   "metadata": {},
   "source": [
    "\n",
    "# Making a Simple Bigram model with Romeo and Juliet Book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d305fd9e-7de7-4d1e-a52c-7ffddd41f250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d95fde-e3cf-4088-840b-9487310ce515",
   "metadata": {},
   "source": [
    "\n",
    "## 1) read the text file with data and store it in a variable\n",
    "\n",
    "    the contents of the book was stored in a `data.txt file` and all it['s content is read and copied to a variable named text.\n",
    "    the length and the first 300 characters was printed to test if it was working and once confirmed moved on the second part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29f2d4b2-192e-4ce0-8c80-71ae53f9da15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142466\n",
      "﻿THE TRAGEDY OF ROMEO AND JULIET\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "THE PROLOGUE.\n",
      "\n",
      "ACT I\n",
      "Scene I. A public place.\n",
      "Scene II. A Street.\n",
      "Scene III. Room in Capulet’s House.\n",
      "Scene IV. A Street.\n",
      "Scene V. A Hall in Capulet’s House.\n",
      "\n",
      "ACT II\n",
      "CHORUS.\n",
      "Scene I. An open place adjoining Capulet’s Garden.\n",
      "Scen\n"
     ]
    }
   ],
   "source": [
    "with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()  \n",
    "print(len(text))\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48074416-f5f4-4c5e-b090-5ddd0c0316bf",
   "metadata": {},
   "source": [
    "\n",
    "## 2) make a sorted set of all the characters in the dataset\n",
    "\n",
    "    a set of all the unique characters in the data was taken using the `set()` method and the result was passed into the `sorted()` method and copied   to the `chars` variable, The result is printed for testing and visualisation purposes. This helps us have a vocabulary of sorts to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a664f27-da77-4582-8a2d-ef9096131346",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '&', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', '—', '‘', '’', '“', '”', '\\ufeff']\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a86e1-2305-40ea-a9ac-8c0155d15abc",
   "metadata": {},
   "source": [
    "\n",
    " ### why do the above ? \n",
    "\n",
    "  this lets us work with tokenizers which comes with encoders and decoders, encoders can help us convert all the 71 characters to numbers. we will be making a character level tokenizer below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbca93b-33c7-4c03-9789-ad1d876ae256",
   "metadata": {},
   "source": [
    "\n",
    "## 3) make a basic character-level tokenizer\n",
    "\n",
    "\n",
    "In order to make the character level tokenizer two dictionaries are first made. **'string_to_integer = { ch:i for i,ch in enumerate(chars) }'** enumerate over each character in the array and assign it an unique number making the first dictionary and reverse for integer to string making the second dictionary .\n",
    "\n",
    "**e.g. :** if  'chars = ['a', 'b', 'c']' then these will be the values of the two dictionaries :\n",
    "string_to_integer = {'a': 0, 'b': 1, 'c': 2}\n",
    "integer_to_string = {0: 'a', 1: 'b', 2: 'c'}\n",
    "\n",
    "the encoder lambda function takes a string s and converts it to a list of integers. For each character c in the string s, it looks up its integer value using the string_to_integer dictionary. The result is a list of integers corresponding to each character.\n",
    "\n",
    "the decoder lambda function takes a list of integers l and converts it back to a string. For each integer i in the list l, it looks up the corresponding character using the integer_to_string dictionary. It then joins these characters into a single string.\n",
    "\n",
    "**another e.g:** \n",
    "\n",
    "encoder('abc')  # e.g. Output: [0, 1, 2]\n",
    "\n",
    "decoder([0, 1, 2])  # e.g. Output: 'abc'\n",
    "\n",
    "these two methods combied helps have the functionality of a basic character-level tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b926ad32-05f5-4e91-8abe-4a17530f5c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 58, 51, 44, 57, 46, 51, 44]\n",
      "yungting\n"
     ]
    }
   ],
   "source": [
    "string_to_integer = { ch:i for i,ch in enumerate(chars) }\n",
    "integer_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encoder = lambda s: [string_to_integer[c] for c in s]\n",
    "decoder = lambda l: ''.join([integer_to_string[i] for i in l])\n",
    "\n",
    "encoded_yungting = encoder('yungting')\n",
    "print(encoded_yungting)\n",
    "decoded_yungting = decoder(encoded_yungting)\n",
    "print(decoded_yungting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a9884-377a-4aa5-88f0-6de60585f8f5",
   "metadata": {},
   "source": [
    "\n",
    "### <ins>More info on tokenizers</ins>\n",
    "\n",
    " ### Types of tokenizers \n",
    "\n",
    " #### 1) Character-level tokenizer\n",
    "\n",
    "- Small Vocabulary but large amount of chcaracters to work with.For example, for the romeo and julirt data we are using we have 142466 characters with a vocabulary of 71 unique characters. There are more characters to encode and decode.\n",
    "\n",
    "\n",
    " #### 2) word-level tokenizer \n",
    "\n",
    "- Very large Vocabulary but smaller amount of words to work with.The vocabulary can get very big especially if there is multiple languages.Less words to encode and decode but very large dictionary\n",
    "\n",
    "#### 3) subword-tokenizer\n",
    "\n",
    "- Somewhere inbetween character-level tokenizer and word-level tokenizer\n",
    "\n",
    "\n",
    "##### it is very important to be efficient with data when working with language models\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79cdf9-dc33-4e84-a038-9555c2f018d9",
   "metadata": {},
   "source": [
    "## 4) Working with tensors \n",
    "\n",
    "#### 1) What are tensors\n",
    "- tensors are the main data structure we will be working with , pytorch does most of the math for it for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bb1e69a-096b-4d47-85d0-be33d5c2a597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3, 4)\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8d3e6-1bcb-4e82-beed-4658bbe2a73c",
   "metadata": {},
   "source": [
    "- as seen above x is of the type Tensor and it is 3 rows and 4 columns and is empty\n",
    "- 1 dimensional tensor is usally called a vector and is reffered to as such\n",
    "- 2 dimensional tensor is usually called a matrix and referred to as such\n",
    "- 3 dimensional and above tensors are usually just referred to as tensors\n",
    "- as per definitions above we can refer to x as a matrix as it is 2 dimensional\n",
    "- x i populated with 32 bit floats by defaults , if you see random values it is just the last values from memory\n",
    "- We use tensors over arrays and stuff as it is easier for pytorch to work with it especially when reshaping, changing dimensionality,multiplying, doing dot products, and so on\n",
    "\n",
    "#### 2) Working with tensors and our enncoder we made \n",
    "\n",
    "\n",
    "- we will use our encoder to encode the entire 'Romeo and Juliet' novel into numbers then we will use torch tensor to make a tensor with the numbers returned from the encoder we made , while making the tensor we can manually set the data type to be sure that the tensor we are making is gonna use the mentioned data type of long\n",
    "- the first 120 characters will be printed out as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bdd9b5f-3acd-461d-9bbb-6f42ccbf4379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([70, 29, 17, 14,  1, 29, 27, 10, 16, 14, 13, 33,  1, 24, 15,  1, 27, 24,\n",
      "        22, 14, 24,  1, 10, 23, 13,  1, 19, 30, 21, 18, 14, 29,  0,  0, 39, 62,\n",
      "         1, 32, 46, 49, 49, 46, 38, 50,  1, 28, 45, 38, 48, 42, 56, 53, 42, 38,\n",
      "        55, 42,  0,  0,  0,  0,  0, 12, 52, 51, 57, 42, 51, 57, 56,  0,  0, 29,\n",
      "        17, 14,  1, 25, 27, 24, 21, 24, 16, 30, 14,  6,  0,  0, 10, 12, 29,  1,\n",
      "        18,  0, 28, 40, 42, 51, 42,  1, 18,  6,  1, 10,  1, 53, 58, 39, 49, 46,\n",
      "        40,  1, 53, 49, 38, 40, 42,  6,  0, 28, 40, 42])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(data[:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df6127-e962-484e-b9d9-32aa35fc1229",
   "metadata": {},
   "source": [
    "## 5) Training and Validation splits\n",
    "\n",
    "-  #### i) why do we need to split our data into training and validation splits ?\n",
    "     - we split it this way as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb40a8f-a4b5-4fab-a4ee-ebc6e1577068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yungting-gpt",
   "language": "python",
   "name": "yugpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
