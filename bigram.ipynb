{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c936ba66-9ff2-487d-8c00-cdb6d7207582",
   "metadata": {},
   "source": [
    "\n",
    "# Making a Simple Bigram model with Romeo and Juliet Book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d305fd9e-7de7-4d1e-a52c-7ffddd41f250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d95fde-e3cf-4088-840b-9487310ce515",
   "metadata": {},
   "source": [
    "\n",
    "## 1) read the text file with data and store it in a variable\n",
    "\n",
    "    the contents of the book was stored in a `data.txt file` and all it['s content is read and copied to a variable named text.\n",
    "    the length and the first 300 characters was printed to test if it was working and once confirmed moved on the second part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f2d4b2-192e-4ce0-8c80-71ae53f9da15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142466\n",
      "﻿THE TRAGEDY OF ROMEO AND JULIET\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "THE PROLOGUE.\n",
      "\n",
      "ACT I\n",
      "Scene I. A public place.\n",
      "Scene II. A Street.\n",
      "Scene III. Room in Capulet’s House.\n",
      "Scene IV. A Street.\n",
      "Scene V. A Hall in Capulet’s House.\n",
      "\n",
      "ACT II\n",
      "CHORUS.\n",
      "Scene I. An open place adjoining Capulet’s Garden.\n",
      "Scen\n"
     ]
    }
   ],
   "source": [
    "with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()  \n",
    "print(len(text))\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48074416-f5f4-4c5e-b090-5ddd0c0316bf",
   "metadata": {},
   "source": [
    "\n",
    "## 2) make a sorted set of all the characters in the dataset\n",
    "\n",
    "    a set of all the unique characters in the data was taken using the `set()` method and the result was passed into the `sorted()` method and copied   to the `chars` variable, The result is printed for testing and visualisation purposes. This helps us have a vocabulary of sorts to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc2dda9-c34e-4a13-b1d6-54fbc1c83aa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '&', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', '—', '‘', '’', '“', '”', '\\ufeff']\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a86e1-2305-40ea-a9ac-8c0155d15abc",
   "metadata": {},
   "source": [
    "\n",
    " ### why do the above ? \n",
    "\n",
    "  this lets us work with tokenizers which comes with encoders and decoders, encoders can help us convert all the 71 characters to numbers. we will be making a character level tokenizer below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbca93b-33c7-4c03-9789-ad1d876ae256",
   "metadata": {},
   "source": [
    "\n",
    "## 3) make a basic character-level tokenizer\n",
    "\n",
    "\n",
    "In order to make the character level tokenizer two dictionaries are first made. **'string_to_integer = { ch:i for i,ch in enumerate(chars) }'** enumerate over each character in the array and assign it an unique number making the first dictionary and reverse for integer to string making the second dictionary .\n",
    "\n",
    "**e.g. :** if  'chars = ['a', 'b', 'c']' then these will be the values of the two dictionaries :\n",
    "string_to_integer = {'a': 0, 'b': 1, 'c': 2}\n",
    "integer_to_string = {0: 'a', 1: 'b', 2: 'c'}\n",
    "\n",
    "the encoder lambda function takes a string s and converts it to a list of integers. For each character c in the string s, it looks up its integer value using the string_to_integer dictionary. The result is a list of integers corresponding to each character.\n",
    "\n",
    "the decoder lambda function takes a list of integers l and converts it back to a string. For each integer i in the list l, it looks up the corresponding character using the integer_to_string dictionary. It then joins these characters into a single string.\n",
    "\n",
    "**another e.g:** \n",
    "\n",
    "encoder('abc')  # e.g. Output: [0, 1, 2]\n",
    "\n",
    "decoder([0, 1, 2])  # e.g. Output: 'abc'\n",
    "\n",
    "these two methods combied helps have the functionality of a basic character-level tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b926ad32-05f5-4e91-8abe-4a17530f5c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 58, 51, 44, 57, 46, 51, 44]\n",
      "yungting\n"
     ]
    }
   ],
   "source": [
    "string_to_integer = { ch:i for i,ch in enumerate(chars) }\n",
    "integer_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encoder = lambda s: [string_to_integer[c] for c in s]\n",
    "decoder = lambda l: ''.join([integer_to_string[i] for i in l])\n",
    "\n",
    "encoded_yungting = encoder('yungting')\n",
    "print(encoded_yungting)\n",
    "decoded_yungting = decoder(encoded_yungting)\n",
    "print(decoded_yungting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a9884-377a-4aa5-88f0-6de60585f8f5",
   "metadata": {},
   "source": [
    "\n",
    "### <ins>More info on tokenizers</ins>\n",
    "\n",
    " ### Types of tokenizers \n",
    "\n",
    " #### 1) Character-level tokenizer\n",
    "\n",
    "- Small Vocabulary but large amount of chcaracters to work with.For example, for the romeo and julirt data we are using we have 142466 characters with a vocabulary of 71 unique characters. There are more characters to encode and decode.\n",
    "\n",
    "\n",
    " #### 2) word-level tokenizer \n",
    "\n",
    "- Very large Vocabulary but smaller amount of words to work with.The vocabulary can get very big especially if there is multiple languages.Less words to encode and decode but very large dictionary\n",
    "\n",
    "#### 3) subword-tokenizer\n",
    "\n",
    "- Somewhere inbetween character-level tokenizer and word-level tokenizer\n",
    "\n",
    "\n",
    "##### it is very important to be efficient with data when working with language models\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79cdf9-dc33-4e84-a038-9555c2f018d9",
   "metadata": {},
   "source": [
    "## 4) Working with tensors \n",
    "\n",
    "#### 1) What are tensors\n",
    "- tensors are the main data structure we will be working with , pytorch does most of the math for it for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb1e69a-096b-4d47-85d0-be33d5c2a597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3,4)\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8d3e6-1bcb-4e82-beed-4658bbe2a73c",
   "metadata": {},
   "source": [
    "- as seen above x is of the type Tensor and it is 3 rows and 4 columns and is empty\n",
    "- 1 dimensional tensor is usally called a vector and is reffered to as such\n",
    "- 2 dimensional tensor is usually called a matrix and referred to as such\n",
    "- 3 dimensional and above tensors are usually just referred to as tensors\n",
    "- as per definitions above we can refer to x as a matrix as it is 2 dimensional\n",
    "- x i populated with 32 bit floats by defaults , if you see random values it is just the last values from memory\n",
    "- We use tensors over arrays and stuff as it is easier for pytorch to work with it especially when reshaping, changing dimensionality,multiplying, doing dot products, and so on\n",
    "\n",
    "#### 2) Working with tensors and our enncoder we made \n",
    "\n",
    "\n",
    "- we will use our encoder to encode the entire 'Romeo and Juliet' novel into numbers then we will use torch tensor to make a tensor with the numbers returned from the encoder we made , while making the tensor we can manually set the data type to be sure that the tensor we are making is gonna use the mentioned data type of long\n",
    "- the first 120 characters will be printed out as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bdd9b5f-3acd-461d-9bbb-6f42ccbf4379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([70, 29, 17, 14,  1, 29, 27, 10, 16, 14, 13, 33,  1, 24, 15,  1, 27, 24,\n",
      "        22, 14, 24,  1, 10, 23, 13,  1, 19, 30, 21, 18, 14, 29,  0,  0, 39, 62,\n",
      "         1, 32, 46, 49, 49, 46, 38, 50,  1, 28, 45, 38, 48, 42, 56, 53, 42, 38,\n",
      "        55, 42,  0,  0,  0,  0,  0, 12, 52, 51, 57, 42, 51, 57, 56,  0,  0, 29,\n",
      "        17, 14,  1, 25, 27, 24, 21, 24, 16, 30, 14,  6,  0,  0, 10, 12, 29,  1,\n",
      "        18,  0, 28, 40, 42, 51, 42,  1, 18,  6,  1, 10,  1, 53, 58, 39, 49, 46,\n",
      "        40,  1, 53, 49, 38, 40, 42,  6,  0, 28, 40, 42])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(data[:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df6127-e962-484e-b9d9-32aa35fc1229",
   "metadata": {},
   "source": [
    "## 5) Training and Validation splits\n",
    "\n",
    "-  #### i) why do we need to split our data into training and validation splits ?\n",
    "     - we split it this way as if we just train the model on 100% of the text after some iterations the model will just memorise the text and would just produce the exact copy of our traiiong text every time, This is not the purpose of training a model as we are trying to train a language model that will complete the text we provide in a similar manner/style to our text from the training data.\n",
    "     - In order to achieve the goal mentioned above we might split our text so that 80% is training data and the rest 20% is validation data.\n",
    "     - e.g. : let's say we are training a model on data that goes sonething like this:\n",
    "         - Yung Ting after infiltrating a maximum security facility in atlantis has decided to make a copy of a top secret recipe that also contains the manfucating process of the viral atlanade drink with the unique...Yung Ting has achieved her goal and atlantis has been added to her list of evergrowing victims.\n",
    "         - let's say the above text is 100 lines of equal tokens per line for the sake of the example. We would use the first 80 lines info as training data and the last 20 lines as validation data making a very simple training and validation split.\n",
    "-  #### ii) how does bigrams come into play in what we are doing here in this simple and basic model ?\n",
    "    - it is called a bigram model as the simple model that is being made here today predicts only based on bigrams, i.e for e.g let's say the model is predicting the word 'yungting' it goes like this (very simple character level example) :\n",
    "        - start -> y\n",
    "        - y -> u\n",
    "        - u -> n\n",
    "        - n -> g\n",
    "        - g -> t\n",
    "        - t -> i\n",
    "        - i -> n\n",
    "        - n -> g\n",
    "    - working in bigrams and would only take the preceding character into account when predicting the next (for now)\n",
    "-  #### iii) how do we train a bigram model to achieve the goals above ? and how are we gonna use an artifical neural network to achieve it\n",
    "    - we would achieve the goals  above by using something called block sizes, we will make **predictions** and **targets** out of them\n",
    "    - e.g. block_size = 7\n",
    "        - context . . . [ 19 , 6 , 25 , 4 , 59 , 20 , 18 ] 21 . . . (predictions)\n",
    "        - context . . . 19 [ 6 , 25 , 4 , 59 , 20 , 18 , 21 ] . . . (targets)\n",
    "        - [] is a tensor in the lines above\n",
    "        - first tensor in python can be [:7] taking the first 7 characters, in the target it can be [1:block_size + 1] making an offset of 1\n",
    "        - we will figure out how far away the predictions are from the target and optimise it so that it is better\n",
    "        - allright now we can do what we just described but in code below starting with the training and validation splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eb40a8f-a4b5-4fab-a4ee-ebc6e1577068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(in characters) when context is ﻿ target value is :  T\n",
      "(In tensors) when context is tensor([70]) target value is :  tensor(29)\n",
      "(in characters) when context is ﻿T target value is :  H\n",
      "(In tensors) when context is tensor([70, 29]) target value is :  tensor(17)\n",
      "(in characters) when context is ﻿TH target value is :  E\n",
      "(In tensors) when context is tensor([70, 29, 17]) target value is :  tensor(14)\n",
      "(in characters) when context is ﻿THE target value is :   \n",
      "(In tensors) when context is tensor([70, 29, 17, 14]) target value is :  tensor(1)\n",
      "(in characters) when context is ﻿THE  target value is :  T\n",
      "(In tensors) when context is tensor([70, 29, 17, 14,  1]) target value is :  tensor(29)\n",
      "(in characters) when context is ﻿THE T target value is :  R\n",
      "(In tensors) when context is tensor([70, 29, 17, 14,  1, 29]) target value is :  tensor(27)\n",
      "(in characters) when context is ﻿THE TR target value is :  A\n",
      "(In tensors) when context is tensor([70, 29, 17, 14,  1, 29, 27]) target value is :  tensor(10)\n"
     ]
    }
   ],
   "source": [
    "s = int(0.7*len(data))\n",
    "training_data = data[:s]\n",
    "validation_data = data[:s]\n",
    "\n",
    "block_size = 7\n",
    "\n",
    "a = training_data[:block_size]\n",
    "b = training_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = a[:t+1]\n",
    "    target = b[t]\n",
    "    print(\"(in characters) when context is\", decoder(context.cpu().detach().numpy()), \"target value is : \",  decoder([target.cpu().detach().numpy().max().item()]) )\n",
    "    print(\"(In tensors) when context is\", context, \"target value is : \", target )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a86be1-f748-49e1-bd4e-67800c173726",
   "metadata": {},
   "source": [
    "## 6) Batches and what are Batch Sizes ? \n",
    "- #### i) what are batches ?\n",
    "    -  what we have done so far is sequential which is not scalable and will be slow, instead what we can do is we can use our gpu and use a lot more cores at onces in parallel using batches. Using our example above we would just have multiple blocks and push it in our gpu for calculation in parallel.\n",
    "    -  without batches performance will be poor\n",
    "- #### ii) what are batch sizes ?   \n",
    "    -  Our batch size is how many blocks we are processing in parallel, and block size is the length of each individual block\n",
    "- #### iii) how do we use batches?\n",
    "    - using batches is very simple with pytorch is very simple and is shown below **(gpu required)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c787bb00-f791-447b-8b7a-99259128f302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d85b67-01c9-4b19-b291-152551a35200",
   "metadata": {},
   "source": [
    "## 7) show yungting the cool stuff pytorch can do in the Pytorch_For_Yung_Ting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb30a4ee-d3ce-4507-b4d6-f555dfb0503f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yungting-gpt",
   "language": "python",
   "name": "yugpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
