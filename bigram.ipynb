{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c936ba66-9ff2-487d-8c00-cdb6d7207582",
   "metadata": {},
   "source": [
    "\n",
    "# Making a Simple Bigram model with Romeo and Juliet Book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d305fd9e-7de7-4d1e-a52c-7ffddd41f250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu124\n",
      "cuda\n",
      "MPS device not found.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "print(torch.__version__)\n",
    "print('cuda') if torch.cuda.is_available() else print('cpu')\n",
    "# for seeing if it works on mac\n",
    "if torch.backends.mps.is_available():\n",
    "    print('mps available')\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(mps_device)\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d95fde-e3cf-4088-840b-9487310ce515",
   "metadata": {},
   "source": [
    "\n",
    "## 1) read the text file with data and store it in a variable\n",
    "\n",
    "    the contents of the book was stored in a `data.txt file` and all it['s content is read and copied to a variable named text.\n",
    "    the length and the first 300 characters was printed to test if it was working and once confirmed moved on the second part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f2d4b2-192e-4ce0-8c80-71ae53f9da15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142466\n",
      "﻿THE TRAGEDY OF ROMEO AND JULIET\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "THE PROLOGUE.\n",
      "\n",
      "ACT I\n",
      "Scene I. A public place.\n",
      "Scene II. A Street.\n",
      "Scene III. Room in Capulet’s House.\n",
      "Scene IV. A Street.\n",
      "Scene V. A Hall in Capulet’s House.\n",
      "\n",
      "ACT II\n",
      "CHORUS.\n",
      "Scene I. An open place adjoining Capulet’s Garden.\n",
      "Scen\n"
     ]
    }
   ],
   "source": [
    "with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()  \n",
    "print(len(text))\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48074416-f5f4-4c5e-b090-5ddd0c0316bf",
   "metadata": {},
   "source": [
    "\n",
    "## 2) make a sorted set of all the characters in the dataset\n",
    "\n",
    "    a set of all the unique characters in the data was taken using the `set()` method and the result was passed into the `sorted()` method and copied   to the `chars` variable, The result is printed for testing and visualisation purposes. This helps us have a vocabulary of sorts to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc2dda9-c34e-4a13-b1d6-54fbc1c83aa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '&', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', '—', '‘', '’', '“', '”', '\\ufeff']\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "print(len(chars))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a86e1-2305-40ea-a9ac-8c0155d15abc",
   "metadata": {},
   "source": [
    "\n",
    " ### why do the above ? \n",
    "\n",
    "  this lets us work with tokenizers which comes with encoders and decoders, encoders can help us convert all the 71 characters to numbers. we will be making a character level tokenizer below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbca93b-33c7-4c03-9789-ad1d876ae256",
   "metadata": {},
   "source": [
    "\n",
    "## 3) make a basic character-level tokenizer\n",
    "\n",
    "\n",
    "In order to make the character level tokenizer two dictionaries are first made. **'string_to_integer = { ch:i for i,ch in enumerate(chars) }'** enumerate over each character in the array and assign it an unique number making the first dictionary and reverse for integer to string making the second dictionary .\n",
    "\n",
    "**e.g. :** if  'chars = ['a', 'b', 'c']' then these will be the values of the two dictionaries :\n",
    "string_to_integer = {'a': 0, 'b': 1, 'c': 2}\n",
    "integer_to_string = {0: 'a', 1: 'b', 2: 'c'}\n",
    "\n",
    "the encoder lambda function takes a string s and converts it to a list of integers. For each character c in the string s, it looks up its integer value using the string_to_integer dictionary. The result is a list of integers corresponding to each character.\n",
    "\n",
    "the decoder lambda function takes a list of integers l and converts it back to a string. For each integer i in the list l, it looks up the corresponding character using the integer_to_string dictionary. It then joins these characters into a single string.\n",
    "\n",
    "**another e.g:** \n",
    "\n",
    "encoder('abc')  # e.g. Output: [0, 1, 2]\n",
    "\n",
    "decoder([0, 1, 2])  # e.g. Output: 'abc'\n",
    "\n",
    "these two methods combied helps have the functionality of a basic character-level tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b926ad32-05f5-4e91-8abe-4a17530f5c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 58, 51, 44, 57, 46, 51, 44]\n",
      "yungting\n"
     ]
    }
   ],
   "source": [
    "string_to_integer = { ch:i for i,ch in enumerate(chars) }\n",
    "integer_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encoder = lambda s: [string_to_integer[c] for c in s]\n",
    "decoder = lambda l: ''.join([integer_to_string[i] for i in l])\n",
    "\n",
    "encoded_yungting = encoder('yungting')\n",
    "print(encoded_yungting)\n",
    "decoded_yungting = decoder(encoded_yungting)\n",
    "print(decoded_yungting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a9884-377a-4aa5-88f0-6de60585f8f5",
   "metadata": {},
   "source": [
    "\n",
    "### <ins>More info on tokenizers</ins>\n",
    "\n",
    " ### Types of tokenizers \n",
    "\n",
    " #### 1) Character-level tokenizer\n",
    "\n",
    "- Small Vocabulary but large amount of chcaracters to work with.For example, for the romeo and julirt data we are using we have 142466 characters with a vocabulary of 71 unique characters. There are more characters to encode and decode.\n",
    "\n",
    "\n",
    " #### 2) word-level tokenizer \n",
    "\n",
    "- Very large Vocabulary but smaller amount of words to work with.The vocabulary can get very big especially if there is multiple languages.Less words to encode and decode but very large dictionary\n",
    "\n",
    "#### 3) subword-tokenizer\n",
    "\n",
    "- Somewhere inbetween character-level tokenizer and word-level tokenizer\n",
    "\n",
    "\n",
    "##### it is very important to be efficient with data when working with language models\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79cdf9-dc33-4e84-a038-9555c2f018d9",
   "metadata": {},
   "source": [
    "## 4) creating and working with tensors \n",
    "\n",
    "#### 1) What are tensors\n",
    "- tensors are the main data structure we will be working with , pytorch does most of the math for it for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb1e69a-096b-4d47-85d0-be33d5c2a597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3,4)\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8d3e6-1bcb-4e82-beed-4658bbe2a73c",
   "metadata": {},
   "source": [
    "- as seen above x is of the type Tensor and it is 3 rows and 4 columns and is empty\n",
    "- 1 dimensional tensor is usally called a vector and is reffered to as such\n",
    "- 2 dimensional tensor is usually called a matrix and referred to as such\n",
    "- 3 dimensional and above tensors are usually just referred to as tensors\n",
    "- as per definitions above we can refer to x as a matrix as it is 2 dimensional\n",
    "- x i populated with 32 bit floats by defaults , if you see random values it is just the last values from memory\n",
    "- We use tensors over arrays and stuff as it is easier for pytorch to work with it especially when reshaping, changing dimensionality,multiplying, doing dot products, and so on\n",
    "\n",
    "#### 2) Working with tensors and our enncoder we made \n",
    "\n",
    "\n",
    "- we will use our encoder to encode the entire 'Romeo and Juliet' novel into numbers then we will use torch tensor to make a tensor with the numbers returned from the encoder we made , while making the tensor we can manually set the data type to be sure that the tensor we are making is gonna use the mentioned data type of long\n",
    "- the first 120 characters will be printed out as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bdd9b5f-3acd-461d-9bbb-6f42ccbf4379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([70, 29, 17, 14,  1, 29, 27, 10, 16, 14, 13, 33,  1, 24, 15,  1, 27, 24,\n",
      "        22, 14, 24,  1, 10, 23, 13,  1, 19, 30, 21, 18, 14, 29,  0,  0, 39, 62,\n",
      "         1, 32, 46, 49, 49, 46, 38, 50,  1, 28, 45, 38, 48, 42, 56, 53, 42, 38,\n",
      "        55, 42,  0,  0,  0,  0,  0, 12, 52, 51, 57, 42, 51, 57, 56,  0,  0, 29,\n",
      "        17, 14,  1, 25, 27, 24, 21, 24, 16, 30, 14,  6,  0,  0, 10, 12, 29,  1,\n",
      "        18,  0, 28, 40, 42, 51, 42,  1, 18,  6,  1, 10,  1, 53, 58, 39, 49, 46,\n",
      "        40,  1, 53, 49, 38, 40, 42,  6,  0, 28, 40, 42])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(data[:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df6127-e962-484e-b9d9-32aa35fc1229",
   "metadata": {},
   "source": [
    "## 5) Creating training and Validation splits\n",
    "\n",
    "-  #### i) why do we need to split our data into training and validation splits ?\n",
    "     - we split it this way as if we just train the model on 100% of the text after some iterations the model will just memorise the text and would just produce the exact copy of our traiiong text every time, This is not the purpose of training a model as we are trying to train a language model that will complete the text we provide in a similar manner/style to our text from the training data.\n",
    "     - In order to achieve the goal mentioned above we might split our text so that 80% is training data and the rest 20% is validation data.\n",
    "     - e.g. : let's say we are training a model on data that goes sonething like this:\n",
    "         - Yung Ting after infiltrating a maximum security facility in atlantis has decided to make a copy of a top secret recipe that also contains the manfucating process of the viral atlanade drink with the unique...Yung Ting has achieved her goal and atlantis has been added to her list of evergrowing victims.\n",
    "         - let's say the above text is 100 lines of equal tokens per line for the sake of the example. We would use the first 80 lines info as training data and the last 20 lines as validation data making a very simple training and validation split.\n",
    "-  #### ii) how does bigrams come into play in what we are doing here in this simple and basic model ?\n",
    "    - it is called a bigram model as the simple model that is being made here today predicts only based on bigrams, i.e for e.g let's say the model is predicting the word 'yungting' it goes like this (very simple character level example) :\n",
    "        - start -> y\n",
    "        - y -> u\n",
    "        - u -> n\n",
    "        - n -> g\n",
    "        - g -> t\n",
    "        - t -> i\n",
    "        - i -> n\n",
    "        - n -> g\n",
    "    - working in bigrams and would only take the preceding character into account when predicting the next (for now)\n",
    "-  #### iii) how do we train a bigram model to achieve the goals above ? and how are we gonna use an artifical neural network to achieve it\n",
    "    - we would achieve the goals  above by using something called block sizes, we will make **predictions** and **targets** out of them\n",
    "    - e.g. block_size = 7\n",
    "        - context . . . [ 19 , 6 , 25 , 4 , 59 , 20 , 18 ] 21 . . . (predictions)\n",
    "        - context . . . 19 [ 6 , 25 , 4 , 59 , 20 , 18 , 21 ] . . . (targets)\n",
    "        - [] is a tensor in the lines above\n",
    "        - first tensor in python can be [:7] taking the first 7 characters, in the target it can be [1:block_size + 1] making an offset of 1\n",
    "        - we will figure out how far away the predictions are from the target and optimise it so that it is better\n",
    "        - allright now we can do what we just described but in code below starting with the training and validation splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eb40a8f-a4b5-4fab-a4ee-ebc6e1577068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(in characters) when context is ﻿ target value is :  T\n",
      "(In tensors) when context is tensor([70]) target value is :  tensor(29)\n",
      "(in characters) when context is ﻿T target value is :  H\n",
      "(In tensors) when context is tensor([70, 29]) target value is :  tensor(17)\n",
      "(in characters) when context is ﻿TH target value is :  E\n",
      "(In tensors) when context is tensor([70, 29, 17]) target value is :  tensor(14)\n",
      "(in characters) when context is ﻿THE target value is :   \n",
      "(In tensors) when context is tensor([70, 29, 17, 14]) target value is :  tensor(1)\n",
      "(in characters) when context is ﻿THE  target value is :  T\n",
      "(In tensors) when context is tensor([70, 29, 17, 14,  1]) target value is :  tensor(29)\n",
      "(in characters) when context is ﻿THE T target value is :  R\n",
      "(In tensors) when context is tensor([70, 29, 17, 14,  1, 29]) target value is :  tensor(27)\n",
      "(in characters) when context is ﻿THE TR target value is :  A\n",
      "(In tensors) when context is tensor([70, 29, 17, 14,  1, 29, 27]) target value is :  tensor(10)\n"
     ]
    }
   ],
   "source": [
    "s = int(0.7*len(data))\n",
    "training_data = data[:s]\n",
    "validation_data = data[:s]\n",
    "\n",
    "block_size = 7\n",
    "\n",
    "a = training_data[:block_size]\n",
    "b = training_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = a[:t+1]\n",
    "    target = b[t]\n",
    "    print(\"(in characters) when context is\", decoder(context.cpu().detach().numpy()), \"target value is : \",  decoder([target.cpu().detach().numpy().max().item()]) )\n",
    "    print(\"(In tensors) when context is\", context, \"target value is : \", target )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a86be1-f748-49e1-bd4e-67800c173726",
   "metadata": {},
   "source": [
    "## 6) Defining the get_batch function\n",
    "- #### i) what are batches ? \n",
    "    -  what we have done so far is sequential which is not scalable and will be slow, instead what we can do is we can use our gpu and use a lot more cores at onces in parallel using batches. Using our example above we would just have multiple blocks and push it in our gpu for calculation in parallel.\n",
    "    -  without batches performance will be poor\n",
    "- #### ii) what are batch sizes ?   \n",
    "    -  Our batch size is how many blocks we are processing in parallel, and block size is the length of each individual block\n",
    "- #### iii) how do we use batches?\n",
    "    - using batches is very simple with pytorch is very simple and is shown below **(gpu required)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c787bb00-f791-447b-8b7a-99259128f302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "inputs: \n",
      "tensor([[23, 31, 24, 21, 18, 24,  6],\n",
      "        [ 1, 18,  1, 38, 50,  1, 57],\n",
      "        [55, 42,  1, 49, 52, 52, 48],\n",
      "        [67, 41,  1, 41, 42, 38, 57],\n",
      "        [42,  1, 44, 52, 52, 56, 42],\n",
      "        [ 0, 16, 42, 51, 57, 49, 42]], device='cuda:0')\n",
      "targets: \n",
      "tensor([[31, 24, 21, 18, 24,  6,  0],\n",
      "        [18,  1, 38, 50,  1, 57, 52],\n",
      "        [42,  1, 49, 52, 52, 48, 42],\n",
      "        [41,  1, 41, 42, 38, 57, 45],\n",
      "        [ 1, 44, 52, 52, 56, 42,  4],\n",
      "        [16, 42, 51, 57, 49, 42, 50]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)\n",
    "batch_size = 6\n",
    "\n",
    "def get_batch(split):\n",
    "    data = training_data if split == 'train' else validation_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs: ')\n",
    "print(x)\n",
    "print('targets: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d85b67-01c9-4b19-b291-152551a35200",
   "metadata": {},
   "source": [
    "\n",
    "- #### iv) more on the get_batch function defined above\n",
    "  - the if statement is there to decide wether to create batch for training data or validation data\n",
    "    - training data as mentioned before is the \"known content\" to train with and validation data is \"unkown\" content we will try and predict\n",
    "    - ix takes a random integer between 0 and length of data - block_size (we get random indices in the text we can start geenrating from)\n",
    "    - we use torch.stack to stack x and y in batches, y is offset from x by 1\n",
    "    - ```x,y = x.to(device), y.to(device)```is used to load the tensors into the gpu (as shown by device = 'cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e644043-1263-4b77-9dfc-99e75b6c704d",
   "metadata": {},
   "source": [
    "## 7) initializing a neural network\n",
    "\n",
    "- #### i) what is gradient descent ?\n",
    "\n",
    "    - Gradient descent helps us minimise the loss by adjusting model parameters itteratively\n",
    "    - how do we find if we are reducing the loss? we can take the derivative at the current point and move it in a direction (let's the slope is increasing in a negative direction so we can keep adjusting the slope in favor of this direction and basically descend with the gradient)\n",
    "    - gradient descents are oprimizers\n",
    "      \n",
    "- #### ii) how is loss calculated ?\n",
    "\n",
    "    - as for how the loss is calculated , let's say we have 80 possible characters with a chance of 1/80 for the prediction we will do -ln(1/80) which around ~4.382 (less than 2% chance)\n",
    "\n",
    "- #### iii) what are optimizers ?\n",
    "\n",
    "    - There are many optimisers, we will use AdamW which combines ideas from momentum and it uses a moving average of both the gradient and it's square parameter to adapt the learning rate of each parameter from Adam and adds weight decay to the adam algorithm (it generalises the parameters more)\n",
    "    - weight decay generalises the perfomance insrtead of having high or low level performance, weight significance will shrink as graph flattens out.\n",
    "    - lets say some weights give insanely high or low performance weight decay , will decay the performance to normalise it.\n",
    "\n",
    "- #### iv) what is Learning Rate ?\n",
    "\n",
    "    -  A hyperparameter that determines the size of the steps taken during optimization.\n",
    "    -  Too high can overshoot minima\n",
    "    -  Too low leads to slow convergence.\n",
    "    -  we wanna have a small learning rate but not too small.\n",
    "\n",
    "- #### v) what is an Embedding Table ?\n",
    "\n",
    "    - A lookup table representing how each token (character) relates to every other token.\n",
    "    - Typically, it’s a matrix of size ```vocab_size x vocab_size```\n",
    "    - shows a probably distribution of what character comes next given one character\n",
    "    - visualisation :\n",
    "    <div>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*RUaIk8LE1vty-J0blNE_eA.png\" width=\"400\"/>\n",
    "    </div>\n",
    "\n",
    "      \n",
    "- #### vi) why a custom forward pass ?\n",
    "\n",
    "    - we do a custom implementation as it is a very good practice when we have specific use cases we know what exactly is going on behind the scenes in the model, what transformations we are doing, how we are storing it and a lot of information that will help us debug.\n",
    "    - we can see how the input is tranformed step by step in each layer to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84757661-7459-4812-bd55-25a083f1ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size , vocab_size)\n",
    "        print(self.token_embedding_table)\n",
    "\n",
    "    def forward_pass(self, index, targets):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        #print(logits)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "        \n",
    "    def generate_tokens(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward_pass(index, targets=None)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718819ff-f5c7-4b23-bd13-943ee42acae3",
   "metadata": {},
   "source": [
    "## more on initialising our neural network\n",
    "- #### what are logits ?\n",
    "    - let's normalise [2,4,6]\n",
    "        - total = 2 + 4 + 6 = 12\n",
    "        -  2 / 12 = 0.167 (16.7% chance)\n",
    "        -  4 / 12 = 0.333 (33.3% chance) \n",
    "        -  6 / 12 = 0.5 (50% chance)\n",
    "        - so a logit is basically [0.167, 0.333, 0.5]\n",
    "        - so let's say first represents ab , second ac , third ad , we know ad is most likely gonna happen next as there is a 50% chance\n",
    "        - basically like a probability distribution\n",
    "- #### what does self.token_embedding_table(index) do?\n",
    "    - For each token in index, it retrieves its embedding (logits)\n",
    "- #### what is the purpose of checking targets ?\n",
    "    - we check if targets is none as we can only calculate the loss if targets are given, if targets is none model is in inference mode (generating text) and no loss is calculated\n",
    "- #### why do we reshape our tensors ?\n",
    "    - we unpack and reshape our tensor due to the input shapes expected by the cross entropy function\n",
    "    - what the cross entropy function expects for input :\n",
    "        - one dimension (C) only number of classes or\n",
    "        - two dimensions (N, C) batch size and number of classes or\n",
    "        - or (N, C, d1, d2, .... dk) with K >= 1 in case of dimensional loss\n",
    "    - we multipy B*T (T is Sequence length which is number of tokens in sequence) to get N and we have C to have the two dimensional input of (N, C)\n",
    "    - the cross entropy function expects (N) for target (or till dk in case of dimensional loss) so we do B*T to get N\n",
    "- #### how does generate_tokens work ?\n",
    "    - the parameter token is the starting token index (shape is (B, T) array of indices in the current context), max_new_tokens is the maximum new tokens to generate\n",
    "    - the new tokens are generated sequentially one at a time till max new tokens are hit\n",
    "    - in each itteration :\n",
    "        - ```logits, loss = self.forward_pass(index, targets=None)``` Computes the logits for the current sequence (targets = None as we are generating tokens , we are not training the model)\n",
    "        - ```logits = logits[:, -1, :]``` Selects the logits corresponding to the last token in each sequence in the batch as in a bigram model the next token prediction depends only on the current (last) token.(as illustrated before) (shape becomes (B, C))\n",
    "        - ```probs = F.softmax(logits, dim=-1)``` Applies the softmax function to convert logits into probabilities. (shape is still (B, C))\n",
    "        - ```index_next = torch.multinomial(probs, num_samples=1)``` Samples the next token index from the probability distribution\n",
    "            - Randomly selects indices based on the provided probabilities. It allows for stochastic sampling, which can introduce variability in generated sequences (shape becomes (B, 1) as it is just the next index so sequence is 1)\n",
    "        - ```index = torch.cat((index, index_next), dim=1)``` Concatenates the newly sampled token to the existing sequence along the sequence length dimension (shape becomes (B, T + 1) we add the next index so sequence increasesn by 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a1699-2e89-442f-af1a-171a1b40f0c0",
   "metadata": {},
   "source": [
    "## 7) testing the generate function\n",
    "- we test the made generate function by generating random characters starting with a 0 tensor resulting in 501 tokens as we generate 500 tokens\n",
    "- we use our decoder to change it back to charcters for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f131a8ba-bb74-48a1-83d8-43c4bc950ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(71, 71)\n",
      "BigramModel(\n",
      "  (token_embedding_table): Embedding(71, 71)\n",
      ")\n",
      "cuda\n",
      "tensor([[0]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "Context device: cuda:0\n",
      "Model device: cuda:0\n",
      "tensor([[ 0, 31, 60, 46, 45, 14,  7, 20, 35, 62, 57,  3, 12, 55,  3, 36, 67, 48,\n",
      "         36, 53, 39, 32, 39,  5, 56,  7, 43, 58, 54,  3, 20, 40, 62, 26, 24, 63,\n",
      "         63, 64,  2, 28, 30, 25,  0, 45, 19, 31, 70, 51, 66,  2,  4, 51, 15, 67,\n",
      "         32, 32, 67, 12, 61, 43,  4, 26,  8, 69,  5, 52, 33, 39, 56,  0, 22,  6,\n",
      "         25, 14, 42, 68,  3, 13,  5, 67, 35, 16, 60, 11,  2, 27,  2, 55, 62, 49,\n",
      "         69,  5, 25, 10, 30, 64, 67,  8, 37, 44, 37, 57, 21, 38, 58,  6, 21, 40,\n",
      "         49, 38, 32, 46, 30, 37,  5, 40, 32,  5, 49, 20, 39,  0, 59, 37, 24, 54,\n",
      "          2, 43, 11, 16, 48, 37, 10, 43, 58, 44, 67, 22, 14, 47, 65, 55,  3, 68,\n",
      "         62, 60, 35, 69,  5,  6, 50,  5,  3, 51,  0, 46, 53,  9,  2, 22,  0,  1,\n",
      "         45, 44, 59, 36,  9, 14, 38,  0, 16, 58, 41, 22,  0, 37, 37, 27, 67, 48,\n",
      "         42, 36, 32, 14,  6, 56, 64, 58, 15, 21, 15, 21, 44, 10, 67, 52, 23, 28,\n",
      "         30, 63, 41, 43, 53, 42,  0, 70, 51, 48, 11, 42, 13,  1, 59, 39, 56, 66,\n",
      "         51, 65, 23, 25, 14, 17, 37, 33, 56,  4, 20, 24, 63, 42, 42,  4, 52,  2,\n",
      "         49, 13, 26, 19, 25, 66, 47, 38, 23, 17, 68, 23, 31, 54, 37, 39, 11, 67,\n",
      "         42,  3, 19,  9, 36, 43, 20, 26, 26, 47,  5, 36, 27, 45, 24, 25, 47, 26,\n",
      "         23,  0, 22,  9, 61, 70, 50, 15, 10, 14, 25, 18, 39, 62,  1, 12, 25, 27,\n",
      "         23,  6, 50,  5, 20,  8,  3, 32,  5, 20, 35, 53, 17, 66, 42, 65, 57, 53,\n",
      "         65, 25,  0, 22, 57, 10, 68, 33,  1, 45, 54, 61, 31, 64, 28, 18, 15, 39,\n",
      "         32,  7, 66, 34, 61, 17,  0, 35, 56, 13, 70, 50, 39, 25, 14, 14, 66, 70,\n",
      "         42, 28, 18, 20, 13, 40, 48, 11, 12, 23, 23, 66, 37, 57,  6,  9, 37, 14,\n",
      "         47, 60, 12, 51, 51,  2, 27, 49, 13, 41, 68, 38,  5, 55, 67, 30,  8, 46,\n",
      "         42, 24, 31,  8, 19, 13, 50, 13, 39, 64, 36, 33, 30, 31, 60, 30, 31, 33,\n",
      "         48, 37, 21, 44, 29, 36, 52, 41, 39, 62,  8, 52,  7, 58, 60, 45, 32, 36,\n",
      "         56, 40,  4,  4, 26,  0, 62, 39, 59, 47, 30, 11, 40, 37,  3, 67, 66,  5,\n",
      "          7, 34, 31, 70, 28, 12, 39, 62, 66,  6,  9, 49, 56, 17, 42, 31,  2,  0,\n",
      "         47, 38, 13, 39, 56, 49, 25, 14, 45,  4, 35, 40, 38,  7, 38, 24, 53, 65,\n",
      "         53, 65, 65, 17, 11, 37, 43, 16, 11, 20, 37, 45, 15, 60,  8, 33, 10, 52,\n",
      "         62, 52, 11, 37, 44, 54, 41, 60, 53, 56, 17,  2, 62, 28, 38]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 501])\n",
      "\n",
      "VwihE:K[yt&Cr&]’k]pbWb-s:fuq&KcyQOzzæ!SUP\n",
      "hJV﻿n‘!,nF’WW’Cxf,Q;”-oYbs\n",
      "M.PEe“&D-’[GwB!R!ryl”-PAUæ’;_g_tLau.LclaWiU_-cW-lKb\n",
      "v_Oq!fBGk_Afug’MEj—r&“yw[”-.m-&n\n",
      "ip?!M\n",
      " hgv]?Ea\n",
      "GudM\n",
      "__R’ke]WE.sæuFLFLgA’oNSUzdfpe\n",
      "﻿nkBeD vbs‘n—NPEH_Ys,KOzee,o!lDQJP‘jaNH“NVq_bB’e&J?]fKQQj-]RhOPjQN\n",
      "M?x﻿mFAEPIby CPRN.m-K;&W-K[pH‘e—tp—P\n",
      "MtA“Y hqxVæSIFbW:‘ZxH\n",
      "[sD﻿mbPEE‘﻿eSIKDckBCNN‘_t.?_EjwCnn!RlDd“a-r’U;ieOV;JDmDbæ]YUVwUVYk_LgT]odby;o:uwhW]sc,,Q\n",
      "ybvjUBc_&’‘-:ZV﻿SCby‘.?lsHeV!\n",
      "jaDbslPEh,[ca:aOp—p——HB_fGBK_hFw;YAoyoB_gqdwpsH!ySa\n"
     ]
    }
   ],
   "source": [
    "model = BigramModel(vocab_size)\n",
    "m = model.to(device)\n",
    "print(model)\n",
    "print(device)\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(context)\n",
    "print(context.shape)\n",
    "print(\"Context device:\", context.device)\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "generated_tokens = m.generate_tokens(context, max_new_tokens=500)\n",
    "print(generated_tokens)\n",
    "print(generated_tokens.shape)\n",
    "generated_chars = decoder(generated_tokens[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059a999-de56-49b1-ae58-fd056f711813",
   "metadata": {},
   "source": [
    "## 8) create a training loop\n",
    "- #### i) create the adamW optimizer using pytorch\n",
    "    - we create a adamW optimizer using our model parameters and the learning rate (determines the size of the steps taken during optimization) we defined\n",
    "        - ##### what are the main kinds of optimisers?\n",
    "            - Mean Squared Error (MSE): MSE is a common loss function used in regression problems, where the goal is to predict a continuous output. It measures the average squared difference between the predicted and actual values, and is often used to train neural networks for regression tasks.\n",
    "            - Gradient Descent (GD): is an optimization algorithm used to minimize the loss function of a machine learning model. The loss function measures how well the model is able to predict the target variable based on the input features. The idea of GD is to iteratively adjust the model parameters in the direction of the steepest descent of the loss function\n",
    "            - Momentum: Momentum is an extension of SGD that adds a \"momentum\" term to the parameter updates. This term helps smooth out the updates and allows the optimizer to continue moving in the right direction, even if the gradient changes direction or varies in magnitude. Momentum is particularly useful for training deep neural networks.\n",
    "            - RMSprop: RMSprop is an optimization algorithm that uses a moving average of the squared gradient to adapt the learning rate of each parameter. This helps to avoid oscillations in the parameter updates and can improve convergence in some cases.\n",
    "            - Adam: Adam is a popular optimization algorithm that combines the ideas of momentum and RMSprop. It uses a moving average of both the gradient and its squared value to adapt the learning rate of each parameter. Adam is often used as a default optimizer for deep learning models.\n",
    "            - AdamW: AdamW is a modification of the Adam optimizer that adds weight decay to the parameter updates. This helps to regularize the model and can improve generalization performance.\n",
    "\n",
    "    - learning rate is something we need to experiment with to figure out the ideal value, if it is too slow it is bad as we would not want to wait months for a simple bigram model , too fast is bad as it can overshoot\n",
    "    - The optimizer updates the model's parameters based on the computed gradients to minimize the loss function.\n",
    "- #### ii) Implement the Training Loop\n",
    "    - Iteratively trains the model by processing batches of data, computing loss, performing backpropagation, and updating model parameters\n",
    "    - ```for iter in range(max_iterations):``` loop runs from 0 to max_iterations - 1\n",
    "    - in each itteration :\n",
    "        - ```inputs, targets = get_batch('train')``` Retrieves a batch of input data (inputs) and corresponding target data (targets) from the training set\n",
    "        - ```logits, loss = model.forward_pass(inputs, targets)```Performs a forward pass through the model, computing the logits (raw predictions) and the loss\n",
    "        - ```optimizer.zero_grad(set_to_none=True)``` Resets the gradients of all optimized parameters to none (instead of 0 due to set_to_none=True)\n",
    "        - ```loss.backward()``` Performs backpropagation, calculating gradients based on the computed loss with respect to each model parameter. (Each parameter's .grad attribute is populated with the gradient of the loss with respect to that parameter)\n",
    "        - ```optimizer.step()``` Updates the model's parameters based on the computed gradients to minimize the loss.(Uses the gradients stored in .grad attributes to perform parameter updates (gradient descent) ,Applies regularization to prevent overfitting by penalizing large weights as we are using AdamW)\n",
    "        - ```if iter % 100 == 0: (new line) print(f\"Iteration {iter}, Loss: {loss.item()}\")``` logs the loss every 100 iterations\n",
    "    - ```print(loss.item())``` print the final loss\n",
    "- #### iii) define the estimate loss function\n",
    "    - we put the model in evaluation mode to estimate losses  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60a5ad42-181a-4c27-a75e-fd239112027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(100)\n",
    "        for k in range(100):\n",
    "            inputs, targets = get_batch(split)\n",
    "            logits, loss = model.forward_pass(inputs, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25871ff3-2b01-4ef6-89d7-61cd6c442e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss 2.42, validation loss 2.43\n",
      "Iteration 100, training loss 2.47, validation loss 2.44\n",
      "Iteration 200, training loss 2.41, validation loss 2.44\n",
      "Iteration 300, training loss 2.45, validation loss 2.42\n",
      "Iteration 400, training loss 2.42, validation loss 2.43\n",
      "Iteration 500, training loss 2.41, validation loss 2.43\n",
      "Iteration 600, training loss 2.41, validation loss 2.47\n",
      "Iteration 700, training loss 2.45, validation loss 2.43\n",
      "Iteration 800, training loss 2.46, validation loss 2.46\n",
      "Iteration 900, training loss 2.40, validation loss 2.41\n",
      "Iteration 1000, training loss 2.44, validation loss 2.42\n",
      "Iteration 1100, training loss 2.40, validation loss 2.44\n",
      "Iteration 1200, training loss 2.43, validation loss 2.45\n",
      "Iteration 1300, training loss 2.42, validation loss 2.41\n",
      "Iteration 1400, training loss 2.45, validation loss 2.41\n",
      "Iteration 1500, training loss 2.41, validation loss 2.43\n",
      "Iteration 1600, training loss 2.44, validation loss 2.40\n",
      "Iteration 1700, training loss 2.45, validation loss 2.43\n",
      "Iteration 1800, training loss 2.40, validation loss 2.42\n",
      "Iteration 1900, training loss 2.45, validation loss 2.46\n",
      "Iteration 2000, training loss 2.45, validation loss 2.44\n",
      "Iteration 2100, training loss 2.42, validation loss 2.46\n",
      "Iteration 2200, training loss 2.40, validation loss 2.41\n",
      "Iteration 2300, training loss 2.42, validation loss 2.45\n",
      "Iteration 2400, training loss 2.41, validation loss 2.40\n",
      "Iteration 2500, training loss 2.43, validation loss 2.43\n",
      "Iteration 2600, training loss 2.42, validation loss 2.42\n",
      "Iteration 2700, training loss 2.39, validation loss 2.44\n",
      "Iteration 2800, training loss 2.42, validation loss 2.42\n",
      "Iteration 2900, training loss 2.47, validation loss 2.44\n",
      "Iteration 3000, training loss 2.42, validation loss 2.44\n",
      "Iteration 3100, training loss 2.43, validation loss 2.43\n",
      "Iteration 3200, training loss 2.42, validation loss 2.43\n",
      "Iteration 3300, training loss 2.44, validation loss 2.42\n",
      "Iteration 3400, training loss 2.41, validation loss 2.41\n",
      "Iteration 3500, training loss 2.41, validation loss 2.42\n",
      "Iteration 3600, training loss 2.41, validation loss 2.44\n",
      "Iteration 3700, training loss 2.42, validation loss 2.44\n",
      "Iteration 3800, training loss 2.43, validation loss 2.42\n",
      "Iteration 3900, training loss 2.44, validation loss 2.45\n",
      "Iteration 4000, training loss 2.43, validation loss 2.45\n",
      "Iteration 4100, training loss 2.43, validation loss 2.40\n",
      "Iteration 4200, training loss 2.42, validation loss 2.43\n",
      "Iteration 4300, training loss 2.44, validation loss 2.42\n",
      "Iteration 4400, training loss 2.42, validation loss 2.41\n",
      "Iteration 4500, training loss 2.42, validation loss 2.43\n",
      "Iteration 4600, training loss 2.41, validation loss 2.41\n",
      "Iteration 4700, training loss 2.42, validation loss 2.44\n",
      "Iteration 4800, training loss 2.41, validation loss 2.43\n",
      "Iteration 4900, training loss 2.45, validation loss 2.44\n",
      "Iteration 5000, training loss 2.43, validation loss 2.43\n",
      "Iteration 5100, training loss 2.44, validation loss 2.42\n",
      "Iteration 5200, training loss 2.47, validation loss 2.44\n",
      "Iteration 5300, training loss 2.39, validation loss 2.41\n",
      "Iteration 5400, training loss 2.45, validation loss 2.42\n",
      "Iteration 5500, training loss 2.46, validation loss 2.44\n",
      "Iteration 5600, training loss 2.45, validation loss 2.44\n",
      "Iteration 5700, training loss 2.42, validation loss 2.42\n",
      "Iteration 5800, training loss 2.40, validation loss 2.42\n",
      "Iteration 5900, training loss 2.41, validation loss 2.42\n",
      "Iteration 6000, training loss 2.43, validation loss 2.40\n",
      "Iteration 6100, training loss 2.41, validation loss 2.45\n",
      "Iteration 6200, training loss 2.43, validation loss 2.43\n",
      "Iteration 6300, training loss 2.42, validation loss 2.41\n",
      "Iteration 6400, training loss 2.43, validation loss 2.44\n",
      "Iteration 6500, training loss 2.42, validation loss 2.42\n",
      "Iteration 6600, training loss 2.42, validation loss 2.47\n",
      "Iteration 6700, training loss 2.44, validation loss 2.42\n",
      "Iteration 6800, training loss 2.41, validation loss 2.42\n",
      "Iteration 6900, training loss 2.42, validation loss 2.43\n",
      "Iteration 7000, training loss 2.42, validation loss 2.44\n",
      "Iteration 7100, training loss 2.44, validation loss 2.47\n",
      "Iteration 7200, training loss 2.40, validation loss 2.45\n",
      "Iteration 7300, training loss 2.44, validation loss 2.44\n",
      "Iteration 7400, training loss 2.44, validation loss 2.41\n",
      "Iteration 7500, training loss 2.42, validation loss 2.40\n",
      "Iteration 7600, training loss 2.45, validation loss 2.41\n",
      "Iteration 7700, training loss 2.44, validation loss 2.45\n",
      "Iteration 7800, training loss 2.43, validation loss 2.41\n",
      "Iteration 7900, training loss 2.44, validation loss 2.44\n",
      "Iteration 8000, training loss 2.42, validation loss 2.40\n",
      "Iteration 8100, training loss 2.43, validation loss 2.43\n",
      "Iteration 8200, training loss 2.44, validation loss 2.40\n",
      "Iteration 8300, training loss 2.43, validation loss 2.43\n",
      "Iteration 8400, training loss 2.43, validation loss 2.42\n",
      "Iteration 8500, training loss 2.41, validation loss 2.41\n",
      "Iteration 8600, training loss 2.45, validation loss 2.43\n",
      "Iteration 8700, training loss 2.45, validation loss 2.39\n",
      "Iteration 8800, training loss 2.41, validation loss 2.46\n",
      "Iteration 8900, training loss 2.47, validation loss 2.41\n",
      "Iteration 9000, training loss 2.45, validation loss 2.44\n",
      "Iteration 9100, training loss 2.40, validation loss 2.44\n",
      "Iteration 9200, training loss 2.40, validation loss 2.39\n",
      "Iteration 9300, training loss 2.45, validation loss 2.42\n",
      "Iteration 9400, training loss 2.44, validation loss 2.43\n",
      "Iteration 9500, training loss 2.44, validation loss 2.43\n",
      "Iteration 9600, training loss 2.43, validation loss 2.42\n",
      "Iteration 9700, training loss 2.43, validation loss 2.43\n",
      "Iteration 9800, training loss 2.42, validation loss 2.45\n",
      "Iteration 9900, training loss 2.40, validation loss 2.40\n",
      "Iteration 10000, training loss 2.47, validation loss 2.43\n",
      "Iteration 10100, training loss 2.45, validation loss 2.44\n",
      "Iteration 10200, training loss 2.43, validation loss 2.42\n",
      "Iteration 10300, training loss 2.43, validation loss 2.45\n",
      "Iteration 10400, training loss 2.44, validation loss 2.43\n",
      "Iteration 10500, training loss 2.41, validation loss 2.46\n",
      "Iteration 10600, training loss 2.42, validation loss 2.42\n",
      "Iteration 10700, training loss 2.44, validation loss 2.45\n",
      "Iteration 10800, training loss 2.43, validation loss 2.43\n",
      "Iteration 10900, training loss 2.40, validation loss 2.43\n",
      "Iteration 11000, training loss 2.41, validation loss 2.44\n",
      "Iteration 11100, training loss 2.45, validation loss 2.44\n",
      "Iteration 11200, training loss 2.40, validation loss 2.42\n",
      "Iteration 11300, training loss 2.43, validation loss 2.44\n",
      "Iteration 11400, training loss 2.40, validation loss 2.43\n",
      "Iteration 11500, training loss 2.41, validation loss 2.42\n",
      "Iteration 11600, training loss 2.43, validation loss 2.43\n",
      "Iteration 11700, training loss 2.43, validation loss 2.39\n",
      "Iteration 11800, training loss 2.41, validation loss 2.41\n",
      "Iteration 11900, training loss 2.42, validation loss 2.42\n",
      "Iteration 12000, training loss 2.43, validation loss 2.45\n",
      "Iteration 12100, training loss 2.46, validation loss 2.43\n",
      "Iteration 12200, training loss 2.45, validation loss 2.43\n",
      "Iteration 12300, training loss 2.47, validation loss 2.43\n",
      "Iteration 12400, training loss 2.43, validation loss 2.42\n",
      "Iteration 12500, training loss 2.42, validation loss 2.44\n",
      "Iteration 12600, training loss 2.42, validation loss 2.43\n",
      "Iteration 12700, training loss 2.42, validation loss 2.42\n",
      "Iteration 12800, training loss 2.45, validation loss 2.44\n",
      "Iteration 12900, training loss 2.40, validation loss 2.44\n",
      "Iteration 13000, training loss 2.44, validation loss 2.46\n",
      "Iteration 13100, training loss 2.41, validation loss 2.41\n",
      "Iteration 13200, training loss 2.42, validation loss 2.39\n",
      "Iteration 13300, training loss 2.42, validation loss 2.43\n",
      "Iteration 13400, training loss 2.44, validation loss 2.41\n",
      "Iteration 13500, training loss 2.40, validation loss 2.45\n",
      "Iteration 13600, training loss 2.43, validation loss 2.44\n",
      "Iteration 13700, training loss 2.43, validation loss 2.42\n",
      "Iteration 13800, training loss 2.45, validation loss 2.42\n",
      "Iteration 13900, training loss 2.45, validation loss 2.42\n",
      "Iteration 14000, training loss 2.41, validation loss 2.45\n",
      "Iteration 14100, training loss 2.41, validation loss 2.42\n",
      "Iteration 14200, training loss 2.45, validation loss 2.44\n",
      "Iteration 14300, training loss 2.42, validation loss 2.42\n",
      "Iteration 14400, training loss 2.43, validation loss 2.43\n",
      "Iteration 14500, training loss 2.43, validation loss 2.40\n",
      "Iteration 14600, training loss 2.44, validation loss 2.42\n",
      "Iteration 14700, training loss 2.43, validation loss 2.43\n",
      "Iteration 14800, training loss 2.41, validation loss 2.42\n",
      "Iteration 14900, training loss 2.42, validation loss 2.45\n",
      "Iteration 15000, training loss 2.42, validation loss 2.42\n",
      "Iteration 15100, training loss 2.43, validation loss 2.44\n",
      "Iteration 15200, training loss 2.43, validation loss 2.45\n",
      "Iteration 15300, training loss 2.43, validation loss 2.45\n",
      "Iteration 15400, training loss 2.42, validation loss 2.47\n",
      "Iteration 15500, training loss 2.41, validation loss 2.42\n",
      "Iteration 15600, training loss 2.41, validation loss 2.41\n",
      "Iteration 15700, training loss 2.41, validation loss 2.42\n",
      "Iteration 15800, training loss 2.43, validation loss 2.43\n",
      "Iteration 15900, training loss 2.41, validation loss 2.45\n",
      "Iteration 16000, training loss 2.39, validation loss 2.39\n",
      "Iteration 16100, training loss 2.43, validation loss 2.43\n",
      "Iteration 16200, training loss 2.38, validation loss 2.44\n",
      "Iteration 16300, training loss 2.42, validation loss 2.45\n",
      "Iteration 16400, training loss 2.43, validation loss 2.43\n",
      "Iteration 16500, training loss 2.39, validation loss 2.44\n",
      "Iteration 16600, training loss 2.41, validation loss 2.42\n",
      "Iteration 16700, training loss 2.43, validation loss 2.41\n",
      "Iteration 16800, training loss 2.43, validation loss 2.41\n",
      "Iteration 16900, training loss 2.44, validation loss 2.43\n",
      "Iteration 17000, training loss 2.46, validation loss 2.42\n",
      "Iteration 17100, training loss 2.43, validation loss 2.46\n",
      "Iteration 17200, training loss 2.44, validation loss 2.43\n",
      "Iteration 17300, training loss 2.42, validation loss 2.42\n",
      "Iteration 17400, training loss 2.43, validation loss 2.42\n",
      "Iteration 17500, training loss 2.43, validation loss 2.41\n",
      "Iteration 17600, training loss 2.45, validation loss 2.43\n",
      "Iteration 17700, training loss 2.42, validation loss 2.43\n",
      "Iteration 17800, training loss 2.43, validation loss 2.40\n",
      "Iteration 17900, training loss 2.44, validation loss 2.44\n",
      "Iteration 18000, training loss 2.45, validation loss 2.40\n",
      "Iteration 18100, training loss 2.43, validation loss 2.40\n",
      "Iteration 18200, training loss 2.45, validation loss 2.42\n",
      "Iteration 18300, training loss 2.42, validation loss 2.41\n",
      "Iteration 18400, training loss 2.43, validation loss 2.45\n",
      "Iteration 18500, training loss 2.44, validation loss 2.44\n",
      "Iteration 18600, training loss 2.42, validation loss 2.47\n",
      "Iteration 18700, training loss 2.46, validation loss 2.44\n",
      "Iteration 18800, training loss 2.43, validation loss 2.45\n",
      "Iteration 18900, training loss 2.44, validation loss 2.48\n",
      "Iteration 19000, training loss 2.45, validation loss 2.45\n",
      "Iteration 19100, training loss 2.43, validation loss 2.42\n",
      "Iteration 19200, training loss 2.39, validation loss 2.46\n",
      "Iteration 19300, training loss 2.45, validation loss 2.44\n",
      "Iteration 19400, training loss 2.45, validation loss 2.40\n",
      "Iteration 19500, training loss 2.45, validation loss 2.44\n",
      "Iteration 19600, training loss 2.42, validation loss 2.42\n",
      "Iteration 19700, training loss 2.43, validation loss 2.46\n",
      "Iteration 19800, training loss 2.38, validation loss 2.44\n",
      "Iteration 19900, training loss 2.43, validation loss 2.42\n",
      "Iteration 20000, training loss 2.44, validation loss 2.44\n",
      "Iteration 20100, training loss 2.41, validation loss 2.43\n",
      "Iteration 20200, training loss 2.44, validation loss 2.42\n",
      "Iteration 20300, training loss 2.45, validation loss 2.42\n",
      "Iteration 20400, training loss 2.40, validation loss 2.42\n",
      "Iteration 20500, training loss 2.45, validation loss 2.43\n",
      "Iteration 20600, training loss 2.44, validation loss 2.44\n",
      "Iteration 20700, training loss 2.40, validation loss 2.43\n",
      "Iteration 20800, training loss 2.43, validation loss 2.42\n",
      "Iteration 20900, training loss 2.46, validation loss 2.43\n",
      "Iteration 21000, training loss 2.43, validation loss 2.42\n",
      "Iteration 21100, training loss 2.45, validation loss 2.43\n",
      "Iteration 21200, training loss 2.41, validation loss 2.44\n",
      "Iteration 21300, training loss 2.42, validation loss 2.44\n",
      "Iteration 21400, training loss 2.41, validation loss 2.42\n",
      "Iteration 21500, training loss 2.42, validation loss 2.41\n",
      "Iteration 21600, training loss 2.44, validation loss 2.42\n",
      "Iteration 21700, training loss 2.42, validation loss 2.42\n",
      "Iteration 21800, training loss 2.46, validation loss 2.45\n",
      "Iteration 21900, training loss 2.42, validation loss 2.44\n",
      "Iteration 22000, training loss 2.42, validation loss 2.44\n",
      "Iteration 22100, training loss 2.44, validation loss 2.39\n",
      "Iteration 22200, training loss 2.46, validation loss 2.44\n",
      "Iteration 22300, training loss 2.42, validation loss 2.42\n",
      "Iteration 22400, training loss 2.44, validation loss 2.43\n",
      "Iteration 22500, training loss 2.43, validation loss 2.44\n",
      "Iteration 22600, training loss 2.40, validation loss 2.40\n",
      "Iteration 22700, training loss 2.42, validation loss 2.42\n",
      "Iteration 22800, training loss 2.43, validation loss 2.40\n",
      "Iteration 22900, training loss 2.44, validation loss 2.45\n",
      "Iteration 23000, training loss 2.43, validation loss 2.44\n",
      "Iteration 23100, training loss 2.43, validation loss 2.44\n",
      "Iteration 23200, training loss 2.42, validation loss 2.44\n",
      "Iteration 23300, training loss 2.45, validation loss 2.44\n",
      "Iteration 23400, training loss 2.41, validation loss 2.42\n",
      "Iteration 23500, training loss 2.44, validation loss 2.43\n",
      "Iteration 23600, training loss 2.41, validation loss 2.45\n",
      "Iteration 23700, training loss 2.39, validation loss 2.40\n",
      "Iteration 23800, training loss 2.43, validation loss 2.42\n",
      "Iteration 23900, training loss 2.43, validation loss 2.47\n",
      "Iteration 24000, training loss 2.43, validation loss 2.45\n",
      "Iteration 24100, training loss 2.44, validation loss 2.43\n",
      "Iteration 24200, training loss 2.42, validation loss 2.41\n",
      "Iteration 24300, training loss 2.44, validation loss 2.45\n",
      "Iteration 24400, training loss 2.44, validation loss 2.43\n",
      "Iteration 24500, training loss 2.41, validation loss 2.45\n",
      "Iteration 24600, training loss 2.45, validation loss 2.45\n",
      "Iteration 24700, training loss 2.43, validation loss 2.43\n",
      "Iteration 24800, training loss 2.44, validation loss 2.41\n",
      "Iteration 24900, training loss 2.42, validation loss 2.42\n",
      "Iteration 25000, training loss 2.42, validation loss 2.44\n",
      "Iteration 25100, training loss 2.42, validation loss 2.42\n",
      "Iteration 25200, training loss 2.41, validation loss 2.40\n",
      "Iteration 25300, training loss 2.42, validation loss 2.43\n",
      "Iteration 25400, training loss 2.40, validation loss 2.41\n",
      "Iteration 25500, training loss 2.43, validation loss 2.41\n",
      "Iteration 25600, training loss 2.43, validation loss 2.44\n",
      "Iteration 25700, training loss 2.42, validation loss 2.43\n",
      "Iteration 25800, training loss 2.46, validation loss 2.43\n",
      "Iteration 25900, training loss 2.41, validation loss 2.43\n",
      "Iteration 26000, training loss 2.42, validation loss 2.43\n",
      "Iteration 26100, training loss 2.46, validation loss 2.43\n",
      "Iteration 26200, training loss 2.44, validation loss 2.42\n",
      "Iteration 26300, training loss 2.42, validation loss 2.45\n",
      "Iteration 26400, training loss 2.42, validation loss 2.44\n",
      "Iteration 26500, training loss 2.42, validation loss 2.42\n",
      "Iteration 26600, training loss 2.46, validation loss 2.43\n",
      "Iteration 26700, training loss 2.47, validation loss 2.41\n",
      "Iteration 26800, training loss 2.43, validation loss 2.40\n",
      "Iteration 26900, training loss 2.43, validation loss 2.40\n",
      "Iteration 27000, training loss 2.43, validation loss 2.42\n",
      "Iteration 27100, training loss 2.42, validation loss 2.41\n",
      "Iteration 27200, training loss 2.40, validation loss 2.40\n",
      "Iteration 27300, training loss 2.40, validation loss 2.41\n",
      "Iteration 27400, training loss 2.41, validation loss 2.45\n",
      "Iteration 27500, training loss 2.40, validation loss 2.41\n",
      "Iteration 27600, training loss 2.45, validation loss 2.45\n",
      "Iteration 27700, training loss 2.41, validation loss 2.44\n",
      "Iteration 27800, training loss 2.41, validation loss 2.42\n",
      "Iteration 27900, training loss 2.44, validation loss 2.44\n",
      "Iteration 28000, training loss 2.45, validation loss 2.41\n",
      "Iteration 28100, training loss 2.44, validation loss 2.41\n",
      "Iteration 28200, training loss 2.43, validation loss 2.45\n",
      "Iteration 28300, training loss 2.43, validation loss 2.41\n",
      "Iteration 28400, training loss 2.44, validation loss 2.44\n",
      "Iteration 28500, training loss 2.43, validation loss 2.45\n",
      "Iteration 28600, training loss 2.43, validation loss 2.40\n",
      "Iteration 28700, training loss 2.43, validation loss 2.42\n",
      "Iteration 28800, training loss 2.44, validation loss 2.43\n",
      "Iteration 28900, training loss 2.41, validation loss 2.44\n",
      "Iteration 29000, training loss 2.41, validation loss 2.42\n",
      "Iteration 29100, training loss 2.43, validation loss 2.41\n",
      "Iteration 29200, training loss 2.42, validation loss 2.42\n",
      "Iteration 29300, training loss 2.43, validation loss 2.42\n",
      "Iteration 29400, training loss 2.42, validation loss 2.42\n",
      "Iteration 29500, training loss 2.42, validation loss 2.41\n",
      "Iteration 29600, training loss 2.41, validation loss 2.41\n",
      "Iteration 29700, training loss 2.42, validation loss 2.43\n",
      "Iteration 29800, training loss 2.43, validation loss 2.40\n",
      "Iteration 29900, training loss 2.44, validation loss 2.45\n",
      "Iteration 30000, training loss 2.42, validation loss 2.45\n",
      "Iteration 30100, training loss 2.40, validation loss 2.43\n",
      "Iteration 30200, training loss 2.46, validation loss 2.42\n",
      "Iteration 30300, training loss 2.44, validation loss 2.45\n",
      "Iteration 30400, training loss 2.44, validation loss 2.42\n",
      "Iteration 30500, training loss 2.44, validation loss 2.44\n",
      "Iteration 30600, training loss 2.42, validation loss 2.43\n",
      "Iteration 30700, training loss 2.41, validation loss 2.44\n",
      "Iteration 30800, training loss 2.41, validation loss 2.43\n",
      "Iteration 30900, training loss 2.43, validation loss 2.45\n",
      "Iteration 31000, training loss 2.44, validation loss 2.44\n",
      "Iteration 31100, training loss 2.44, validation loss 2.42\n",
      "Iteration 31200, training loss 2.39, validation loss 2.43\n",
      "Iteration 31300, training loss 2.42, validation loss 2.39\n",
      "Iteration 31400, training loss 2.40, validation loss 2.40\n",
      "Iteration 31500, training loss 2.47, validation loss 2.44\n",
      "Iteration 31600, training loss 2.41, validation loss 2.43\n",
      "Iteration 31700, training loss 2.44, validation loss 2.43\n",
      "Iteration 31800, training loss 2.41, validation loss 2.41\n",
      "Iteration 31900, training loss 2.43, validation loss 2.43\n",
      "Iteration 32000, training loss 2.46, validation loss 2.43\n",
      "Iteration 32100, training loss 2.41, validation loss 2.42\n",
      "Iteration 32200, training loss 2.44, validation loss 2.43\n",
      "Iteration 32300, training loss 2.45, validation loss 2.43\n",
      "Iteration 32400, training loss 2.44, validation loss 2.43\n",
      "Iteration 32500, training loss 2.43, validation loss 2.42\n",
      "Iteration 32600, training loss 2.42, validation loss 2.40\n",
      "Iteration 32700, training loss 2.43, validation loss 2.43\n",
      "Iteration 32800, training loss 2.46, validation loss 2.43\n",
      "Iteration 32900, training loss 2.42, validation loss 2.41\n",
      "Iteration 33000, training loss 2.44, validation loss 2.43\n",
      "Iteration 33100, training loss 2.42, validation loss 2.42\n",
      "Iteration 33200, training loss 2.45, validation loss 2.42\n",
      "Iteration 33300, training loss 2.43, validation loss 2.42\n",
      "Iteration 33400, training loss 2.42, validation loss 2.44\n",
      "Iteration 33500, training loss 2.39, validation loss 2.44\n",
      "Iteration 33600, training loss 2.40, validation loss 2.40\n",
      "Iteration 33700, training loss 2.42, validation loss 2.42\n",
      "Iteration 33800, training loss 2.45, validation loss 2.43\n",
      "Iteration 33900, training loss 2.42, validation loss 2.45\n",
      "Iteration 34000, training loss 2.42, validation loss 2.44\n",
      "Iteration 34100, training loss 2.44, validation loss 2.45\n",
      "Iteration 34200, training loss 2.40, validation loss 2.43\n",
      "Iteration 34300, training loss 2.42, validation loss 2.43\n",
      "Iteration 34400, training loss 2.43, validation loss 2.45\n",
      "Iteration 34500, training loss 2.42, validation loss 2.44\n",
      "Iteration 34600, training loss 2.39, validation loss 2.45\n",
      "Iteration 34700, training loss 2.44, validation loss 2.42\n",
      "Iteration 34800, training loss 2.42, validation loss 2.42\n",
      "Iteration 34900, training loss 2.43, validation loss 2.47\n",
      "Iteration 35000, training loss 2.43, validation loss 2.41\n",
      "Iteration 35100, training loss 2.41, validation loss 2.44\n",
      "Iteration 35200, training loss 2.42, validation loss 2.41\n",
      "Iteration 35300, training loss 2.42, validation loss 2.47\n",
      "Iteration 35400, training loss 2.38, validation loss 2.45\n",
      "Iteration 35500, training loss 2.42, validation loss 2.42\n",
      "Iteration 35600, training loss 2.44, validation loss 2.45\n",
      "Iteration 35700, training loss 2.45, validation loss 2.43\n",
      "Iteration 35800, training loss 2.43, validation loss 2.43\n",
      "Iteration 35900, training loss 2.44, validation loss 2.41\n",
      "Iteration 36000, training loss 2.42, validation loss 2.44\n",
      "Iteration 36100, training loss 2.41, validation loss 2.41\n",
      "Iteration 36200, training loss 2.45, validation loss 2.45\n",
      "Iteration 36300, training loss 2.42, validation loss 2.43\n",
      "Iteration 36400, training loss 2.47, validation loss 2.42\n",
      "Iteration 36500, training loss 2.42, validation loss 2.43\n",
      "Iteration 36600, training loss 2.43, validation loss 2.42\n",
      "Iteration 36700, training loss 2.45, validation loss 2.43\n",
      "Iteration 36800, training loss 2.43, validation loss 2.43\n",
      "Iteration 36900, training loss 2.44, validation loss 2.45\n",
      "Iteration 37000, training loss 2.42, validation loss 2.42\n",
      "Iteration 37100, training loss 2.44, validation loss 2.43\n",
      "Iteration 37200, training loss 2.41, validation loss 2.41\n",
      "Iteration 37300, training loss 2.42, validation loss 2.41\n",
      "Iteration 37400, training loss 2.41, validation loss 2.43\n",
      "Iteration 37500, training loss 2.44, validation loss 2.45\n",
      "Iteration 37600, training loss 2.43, validation loss 2.44\n",
      "Iteration 37700, training loss 2.45, validation loss 2.45\n",
      "Iteration 37800, training loss 2.38, validation loss 2.45\n",
      "Iteration 37900, training loss 2.44, validation loss 2.40\n",
      "Iteration 38000, training loss 2.43, validation loss 2.44\n",
      "Iteration 38100, training loss 2.44, validation loss 2.44\n",
      "Iteration 38200, training loss 2.42, validation loss 2.39\n",
      "Iteration 38300, training loss 2.45, validation loss 2.40\n",
      "Iteration 38400, training loss 2.45, validation loss 2.40\n",
      "Iteration 38500, training loss 2.43, validation loss 2.43\n",
      "Iteration 38600, training loss 2.40, validation loss 2.42\n",
      "Iteration 38700, training loss 2.39, validation loss 2.42\n",
      "Iteration 38800, training loss 2.42, validation loss 2.43\n",
      "Iteration 38900, training loss 2.42, validation loss 2.44\n",
      "Iteration 39000, training loss 2.41, validation loss 2.43\n",
      "Iteration 39100, training loss 2.44, validation loss 2.43\n",
      "Iteration 39200, training loss 2.43, validation loss 2.44\n",
      "Iteration 39300, training loss 2.40, validation loss 2.41\n",
      "Iteration 39400, training loss 2.42, validation loss 2.47\n",
      "Iteration 39500, training loss 2.45, validation loss 2.42\n",
      "Iteration 39600, training loss 2.45, validation loss 2.46\n",
      "Iteration 39700, training loss 2.42, validation loss 2.39\n",
      "Iteration 39800, training loss 2.43, validation loss 2.43\n",
      "Iteration 39900, training loss 2.40, validation loss 2.48\n",
      "Iteration 40000, training loss 2.43, validation loss 2.46\n",
      "Iteration 40100, training loss 2.43, validation loss 2.42\n",
      "Iteration 40200, training loss 2.43, validation loss 2.43\n",
      "Iteration 40300, training loss 2.48, validation loss 2.42\n",
      "Iteration 40400, training loss 2.44, validation loss 2.43\n",
      "Iteration 40500, training loss 2.40, validation loss 2.43\n",
      "Iteration 40600, training loss 2.44, validation loss 2.47\n",
      "Iteration 40700, training loss 2.42, validation loss 2.41\n",
      "Iteration 40800, training loss 2.43, validation loss 2.43\n",
      "Iteration 40900, training loss 2.44, validation loss 2.39\n",
      "Iteration 41000, training loss 2.44, validation loss 2.41\n",
      "Iteration 41100, training loss 2.43, validation loss 2.43\n",
      "Iteration 41200, training loss 2.40, validation loss 2.46\n",
      "Iteration 41300, training loss 2.43, validation loss 2.46\n",
      "Iteration 41400, training loss 2.45, validation loss 2.44\n",
      "Iteration 41500, training loss 2.44, validation loss 2.42\n",
      "Iteration 41600, training loss 2.44, validation loss 2.44\n",
      "Iteration 41700, training loss 2.45, validation loss 2.43\n",
      "Iteration 41800, training loss 2.39, validation loss 2.41\n",
      "Iteration 41900, training loss 2.41, validation loss 2.45\n",
      "Iteration 42000, training loss 2.44, validation loss 2.40\n",
      "Iteration 42100, training loss 2.45, validation loss 2.41\n",
      "Iteration 42200, training loss 2.42, validation loss 2.44\n",
      "Iteration 42300, training loss 2.42, validation loss 2.44\n",
      "Iteration 42400, training loss 2.41, validation loss 2.42\n",
      "Iteration 42500, training loss 2.41, validation loss 2.44\n",
      "Iteration 42600, training loss 2.44, validation loss 2.42\n",
      "Iteration 42700, training loss 2.44, validation loss 2.46\n",
      "Iteration 42800, training loss 2.45, validation loss 2.43\n",
      "Iteration 42900, training loss 2.39, validation loss 2.44\n",
      "Iteration 43000, training loss 2.43, validation loss 2.42\n",
      "Iteration 43100, training loss 2.41, validation loss 2.42\n",
      "Iteration 43200, training loss 2.42, validation loss 2.41\n",
      "Iteration 43300, training loss 2.45, validation loss 2.40\n",
      "Iteration 43400, training loss 2.42, validation loss 2.45\n",
      "Iteration 43500, training loss 2.40, validation loss 2.46\n",
      "Iteration 43600, training loss 2.43, validation loss 2.44\n",
      "Iteration 43700, training loss 2.42, validation loss 2.42\n",
      "Iteration 43800, training loss 2.40, validation loss 2.44\n",
      "Iteration 43900, training loss 2.40, validation loss 2.42\n",
      "Iteration 44000, training loss 2.44, validation loss 2.42\n",
      "Iteration 44100, training loss 2.43, validation loss 2.44\n",
      "Iteration 44200, training loss 2.40, validation loss 2.45\n",
      "Iteration 44300, training loss 2.44, validation loss 2.43\n",
      "Iteration 44400, training loss 2.45, validation loss 2.42\n",
      "Iteration 44500, training loss 2.42, validation loss 2.42\n",
      "Iteration 44600, training loss 2.44, validation loss 2.44\n",
      "Iteration 44700, training loss 2.45, validation loss 2.41\n",
      "Iteration 44800, training loss 2.40, validation loss 2.41\n",
      "Iteration 44900, training loss 2.42, validation loss 2.43\n",
      "Iteration 45000, training loss 2.40, validation loss 2.42\n",
      "Iteration 45100, training loss 2.44, validation loss 2.42\n",
      "Iteration 45200, training loss 2.41, validation loss 2.44\n",
      "Iteration 45300, training loss 2.43, validation loss 2.41\n",
      "Iteration 45400, training loss 2.43, validation loss 2.44\n",
      "Iteration 45500, training loss 2.43, validation loss 2.43\n",
      "Iteration 45600, training loss 2.42, validation loss 2.44\n",
      "Iteration 45700, training loss 2.40, validation loss 2.42\n",
      "Iteration 45800, training loss 2.41, validation loss 2.41\n",
      "Iteration 45900, training loss 2.43, validation loss 2.42\n",
      "Iteration 46000, training loss 2.44, validation loss 2.41\n",
      "Iteration 46100, training loss 2.42, validation loss 2.43\n",
      "Iteration 46200, training loss 2.44, validation loss 2.41\n",
      "Iteration 46300, training loss 2.38, validation loss 2.41\n",
      "Iteration 46400, training loss 2.41, validation loss 2.45\n",
      "Iteration 46500, training loss 2.44, validation loss 2.42\n",
      "Iteration 46600, training loss 2.45, validation loss 2.46\n",
      "Iteration 46700, training loss 2.43, validation loss 2.44\n",
      "Iteration 46800, training loss 2.42, validation loss 2.43\n",
      "Iteration 46900, training loss 2.42, validation loss 2.40\n",
      "Iteration 47000, training loss 2.45, validation loss 2.41\n",
      "Iteration 47100, training loss 2.42, validation loss 2.41\n",
      "Iteration 47200, training loss 2.41, validation loss 2.43\n",
      "Iteration 47300, training loss 2.43, validation loss 2.42\n",
      "Iteration 47400, training loss 2.44, validation loss 2.46\n",
      "Iteration 47500, training loss 2.42, validation loss 2.39\n",
      "Iteration 47600, training loss 2.42, validation loss 2.42\n",
      "Iteration 47700, training loss 2.42, validation loss 2.43\n",
      "Iteration 47800, training loss 2.41, validation loss 2.46\n",
      "Iteration 47900, training loss 2.46, validation loss 2.43\n",
      "Iteration 48000, training loss 2.41, validation loss 2.45\n",
      "Iteration 48100, training loss 2.41, validation loss 2.43\n",
      "Iteration 48200, training loss 2.42, validation loss 2.45\n",
      "Iteration 48300, training loss 2.45, validation loss 2.42\n",
      "Iteration 48400, training loss 2.43, validation loss 2.41\n",
      "Iteration 48500, training loss 2.44, validation loss 2.41\n",
      "Iteration 48600, training loss 2.43, validation loss 2.42\n",
      "Iteration 48700, training loss 2.45, validation loss 2.40\n",
      "Iteration 48800, training loss 2.40, validation loss 2.45\n",
      "Iteration 48900, training loss 2.41, validation loss 2.45\n",
      "Iteration 49000, training loss 2.42, validation loss 2.44\n",
      "Iteration 49100, training loss 2.44, validation loss 2.43\n",
      "Iteration 49200, training loss 2.44, validation loss 2.46\n",
      "Iteration 49300, training loss 2.41, validation loss 2.46\n",
      "Iteration 49400, training loss 2.43, validation loss 2.45\n",
      "Iteration 49500, training loss 2.41, validation loss 2.39\n",
      "Iteration 49600, training loss 2.45, validation loss 2.42\n",
      "Iteration 49700, training loss 2.42, validation loss 2.42\n",
      "Iteration 49800, training loss 2.43, validation loss 2.43\n",
      "Iteration 49900, training loss 2.39, validation loss 2.41\n",
      "Iteration 50000, training loss 2.44, validation loss 2.46\n",
      "Iteration 50100, training loss 2.45, validation loss 2.41\n",
      "Iteration 50200, training loss 2.46, validation loss 2.43\n",
      "Iteration 50300, training loss 2.45, validation loss 2.43\n",
      "Iteration 50400, training loss 2.39, validation loss 2.45\n",
      "Iteration 50500, training loss 2.43, validation loss 2.44\n",
      "Iteration 50600, training loss 2.41, validation loss 2.44\n",
      "Iteration 50700, training loss 2.45, validation loss 2.41\n",
      "Iteration 50800, training loss 2.42, validation loss 2.43\n",
      "Iteration 50900, training loss 2.44, validation loss 2.42\n",
      "Iteration 51000, training loss 2.42, validation loss 2.43\n",
      "Iteration 51100, training loss 2.41, validation loss 2.42\n",
      "Iteration 51200, training loss 2.42, validation loss 2.45\n",
      "Iteration 51300, training loss 2.42, validation loss 2.39\n",
      "Iteration 51400, training loss 2.44, validation loss 2.44\n",
      "Iteration 51500, training loss 2.43, validation loss 2.43\n",
      "Iteration 51600, training loss 2.43, validation loss 2.46\n",
      "Iteration 51700, training loss 2.43, validation loss 2.44\n",
      "Iteration 51800, training loss 2.41, validation loss 2.42\n",
      "Iteration 51900, training loss 2.43, validation loss 2.42\n",
      "Iteration 52000, training loss 2.44, validation loss 2.41\n",
      "Iteration 52100, training loss 2.46, validation loss 2.44\n",
      "Iteration 52200, training loss 2.40, validation loss 2.47\n",
      "Iteration 52300, training loss 2.40, validation loss 2.44\n",
      "Iteration 52400, training loss 2.42, validation loss 2.41\n",
      "Iteration 52500, training loss 2.45, validation loss 2.44\n",
      "Iteration 52600, training loss 2.43, validation loss 2.43\n",
      "Iteration 52700, training loss 2.41, validation loss 2.43\n",
      "Iteration 52800, training loss 2.42, validation loss 2.42\n",
      "Iteration 52900, training loss 2.44, validation loss 2.43\n",
      "Iteration 53000, training loss 2.44, validation loss 2.41\n",
      "Iteration 53100, training loss 2.41, validation loss 2.41\n",
      "Iteration 53200, training loss 2.46, validation loss 2.41\n",
      "Iteration 53300, training loss 2.42, validation loss 2.44\n",
      "Iteration 53400, training loss 2.43, validation loss 2.42\n",
      "Iteration 53500, training loss 2.43, validation loss 2.42\n",
      "Iteration 53600, training loss 2.44, validation loss 2.38\n",
      "Iteration 53700, training loss 2.43, validation loss 2.45\n",
      "Iteration 53800, training loss 2.40, validation loss 2.45\n",
      "Iteration 53900, training loss 2.39, validation loss 2.40\n",
      "Iteration 54000, training loss 2.40, validation loss 2.43\n",
      "Iteration 54100, training loss 2.42, validation loss 2.41\n",
      "Iteration 54200, training loss 2.41, validation loss 2.43\n",
      "Iteration 54300, training loss 2.44, validation loss 2.49\n",
      "Iteration 54400, training loss 2.43, validation loss 2.44\n",
      "Iteration 54500, training loss 2.43, validation loss 2.42\n",
      "Iteration 54600, training loss 2.43, validation loss 2.46\n",
      "Iteration 54700, training loss 2.42, validation loss 2.48\n",
      "Iteration 54800, training loss 2.46, validation loss 2.43\n",
      "Iteration 54900, training loss 2.43, validation loss 2.42\n",
      "Iteration 55000, training loss 2.42, validation loss 2.40\n",
      "Iteration 55100, training loss 2.44, validation loss 2.43\n",
      "Iteration 55200, training loss 2.45, validation loss 2.42\n",
      "Iteration 55300, training loss 2.43, validation loss 2.40\n",
      "Iteration 55400, training loss 2.44, validation loss 2.41\n",
      "Iteration 55500, training loss 2.44, validation loss 2.41\n",
      "Iteration 55600, training loss 2.40, validation loss 2.43\n",
      "Iteration 55700, training loss 2.44, validation loss 2.44\n",
      "Iteration 55800, training loss 2.45, validation loss 2.46\n",
      "Iteration 55900, training loss 2.44, validation loss 2.43\n",
      "Iteration 56000, training loss 2.42, validation loss 2.41\n",
      "Iteration 56100, training loss 2.43, validation loss 2.44\n",
      "Iteration 56200, training loss 2.48, validation loss 2.41\n",
      "Iteration 56300, training loss 2.42, validation loss 2.41\n",
      "Iteration 56400, training loss 2.43, validation loss 2.42\n",
      "Iteration 56500, training loss 2.41, validation loss 2.43\n",
      "Iteration 56600, training loss 2.43, validation loss 2.42\n",
      "Iteration 56700, training loss 2.44, validation loss 2.42\n",
      "Iteration 56800, training loss 2.43, validation loss 2.46\n",
      "Iteration 56900, training loss 2.41, validation loss 2.41\n",
      "Iteration 57000, training loss 2.47, validation loss 2.41\n",
      "Iteration 57100, training loss 2.45, validation loss 2.44\n",
      "Iteration 57200, training loss 2.42, validation loss 2.42\n",
      "Iteration 57300, training loss 2.42, validation loss 2.43\n",
      "Iteration 57400, training loss 2.42, validation loss 2.42\n",
      "Iteration 57500, training loss 2.41, validation loss 2.43\n",
      "Iteration 57600, training loss 2.46, validation loss 2.43\n",
      "Iteration 57700, training loss 2.43, validation loss 2.44\n",
      "Iteration 57800, training loss 2.45, validation loss 2.44\n",
      "Iteration 57900, training loss 2.42, validation loss 2.46\n",
      "Iteration 58000, training loss 2.40, validation loss 2.43\n",
      "Iteration 58100, training loss 2.44, validation loss 2.44\n",
      "Iteration 58200, training loss 2.42, validation loss 2.42\n",
      "Iteration 58300, training loss 2.42, validation loss 2.41\n",
      "Iteration 58400, training loss 2.42, validation loss 2.41\n",
      "Iteration 58500, training loss 2.45, validation loss 2.44\n",
      "Iteration 58600, training loss 2.41, validation loss 2.43\n",
      "Iteration 58700, training loss 2.42, validation loss 2.41\n",
      "Iteration 58800, training loss 2.42, validation loss 2.43\n",
      "Iteration 58900, training loss 2.41, validation loss 2.46\n",
      "Iteration 59000, training loss 2.43, validation loss 2.41\n",
      "Iteration 59100, training loss 2.42, validation loss 2.42\n",
      "Iteration 59200, training loss 2.44, validation loss 2.42\n",
      "Iteration 59300, training loss 2.42, validation loss 2.42\n",
      "Iteration 59400, training loss 2.42, validation loss 2.43\n",
      "Iteration 59500, training loss 2.45, validation loss 2.40\n",
      "Iteration 59600, training loss 2.43, validation loss 2.43\n",
      "Iteration 59700, training loss 2.43, validation loss 2.42\n",
      "Iteration 59800, training loss 2.44, validation loss 2.41\n",
      "Iteration 59900, training loss 2.43, validation loss 2.43\n",
      "Iteration 60000, training loss 2.40, validation loss 2.46\n",
      "Iteration 60100, training loss 2.41, validation loss 2.41\n",
      "Iteration 60200, training loss 2.42, validation loss 2.45\n",
      "Iteration 60300, training loss 2.46, validation loss 2.42\n",
      "Iteration 60400, training loss 2.44, validation loss 2.44\n",
      "Iteration 60500, training loss 2.39, validation loss 2.45\n",
      "Iteration 60600, training loss 2.41, validation loss 2.41\n",
      "Iteration 60700, training loss 2.42, validation loss 2.42\n",
      "Iteration 60800, training loss 2.38, validation loss 2.43\n",
      "Iteration 60900, training loss 2.45, validation loss 2.42\n",
      "Iteration 61000, training loss 2.39, validation loss 2.44\n",
      "Iteration 61100, training loss 2.41, validation loss 2.45\n",
      "Iteration 61200, training loss 2.40, validation loss 2.43\n",
      "Iteration 61300, training loss 2.42, validation loss 2.44\n",
      "Iteration 61400, training loss 2.41, validation loss 2.40\n",
      "Iteration 61500, training loss 2.44, validation loss 2.42\n",
      "Iteration 61600, training loss 2.40, validation loss 2.42\n",
      "Iteration 61700, training loss 2.44, validation loss 2.43\n",
      "Iteration 61800, training loss 2.43, validation loss 2.44\n",
      "Iteration 61900, training loss 2.46, validation loss 2.43\n",
      "Iteration 62000, training loss 2.43, validation loss 2.45\n",
      "Iteration 62100, training loss 2.45, validation loss 2.43\n",
      "Iteration 62200, training loss 2.43, validation loss 2.41\n",
      "Iteration 62300, training loss 2.40, validation loss 2.39\n",
      "Iteration 62400, training loss 2.45, validation loss 2.41\n",
      "Iteration 62500, training loss 2.43, validation loss 2.43\n",
      "Iteration 62600, training loss 2.44, validation loss 2.45\n",
      "Iteration 62700, training loss 2.44, validation loss 2.40\n",
      "Iteration 62800, training loss 2.42, validation loss 2.47\n",
      "Iteration 62900, training loss 2.44, validation loss 2.40\n",
      "Iteration 63000, training loss 2.39, validation loss 2.42\n",
      "Iteration 63100, training loss 2.44, validation loss 2.40\n",
      "Iteration 63200, training loss 2.40, validation loss 2.42\n",
      "Iteration 63300, training loss 2.42, validation loss 2.42\n",
      "Iteration 63400, training loss 2.44, validation loss 2.40\n",
      "Iteration 63500, training loss 2.48, validation loss 2.44\n",
      "Iteration 63600, training loss 2.42, validation loss 2.43\n",
      "Iteration 63700, training loss 2.41, validation loss 2.42\n",
      "Iteration 63800, training loss 2.43, validation loss 2.44\n",
      "Iteration 63900, training loss 2.43, validation loss 2.45\n",
      "Iteration 64000, training loss 2.43, validation loss 2.41\n",
      "Iteration 64100, training loss 2.41, validation loss 2.43\n",
      "Iteration 64200, training loss 2.42, validation loss 2.45\n",
      "Iteration 64300, training loss 2.42, validation loss 2.41\n",
      "Iteration 64400, training loss 2.44, validation loss 2.41\n",
      "Iteration 64500, training loss 2.44, validation loss 2.44\n",
      "Iteration 64600, training loss 2.46, validation loss 2.40\n",
      "Iteration 64700, training loss 2.45, validation loss 2.44\n",
      "Iteration 64800, training loss 2.42, validation loss 2.42\n",
      "Iteration 64900, training loss 2.43, validation loss 2.41\n",
      "Iteration 65000, training loss 2.43, validation loss 2.41\n",
      "Iteration 65100, training loss 2.42, validation loss 2.41\n",
      "Iteration 65200, training loss 2.44, validation loss 2.42\n",
      "Iteration 65300, training loss 2.44, validation loss 2.42\n",
      "Iteration 65400, training loss 2.43, validation loss 2.42\n",
      "Iteration 65500, training loss 2.43, validation loss 2.43\n",
      "Iteration 65600, training loss 2.45, validation loss 2.43\n",
      "Iteration 65700, training loss 2.45, validation loss 2.43\n",
      "Iteration 65800, training loss 2.46, validation loss 2.42\n",
      "Iteration 65900, training loss 2.44, validation loss 2.42\n",
      "Iteration 66000, training loss 2.43, validation loss 2.42\n",
      "Iteration 66100, training loss 2.42, validation loss 2.45\n",
      "Iteration 66200, training loss 2.42, validation loss 2.45\n",
      "Iteration 66300, training loss 2.41, validation loss 2.40\n",
      "Iteration 66400, training loss 2.44, validation loss 2.43\n",
      "Iteration 66500, training loss 2.44, validation loss 2.42\n",
      "Iteration 66600, training loss 2.42, validation loss 2.45\n",
      "Iteration 66700, training loss 2.42, validation loss 2.43\n",
      "Iteration 66800, training loss 2.41, validation loss 2.42\n",
      "Iteration 66900, training loss 2.41, validation loss 2.42\n",
      "Iteration 67000, training loss 2.42, validation loss 2.44\n",
      "Iteration 67100, training loss 2.42, validation loss 2.42\n",
      "Iteration 67200, training loss 2.42, validation loss 2.43\n",
      "Iteration 67300, training loss 2.42, validation loss 2.48\n",
      "Iteration 67400, training loss 2.43, validation loss 2.43\n",
      "Iteration 67500, training loss 2.44, validation loss 2.43\n",
      "Iteration 67600, training loss 2.42, validation loss 2.45\n",
      "Iteration 67700, training loss 2.42, validation loss 2.43\n",
      "Iteration 67800, training loss 2.44, validation loss 2.45\n",
      "Iteration 67900, training loss 2.45, validation loss 2.42\n",
      "Iteration 68000, training loss 2.43, validation loss 2.43\n",
      "Iteration 68100, training loss 2.41, validation loss 2.44\n",
      "Iteration 68200, training loss 2.43, validation loss 2.43\n",
      "Iteration 68300, training loss 2.42, validation loss 2.41\n",
      "Iteration 68400, training loss 2.44, validation loss 2.43\n",
      "Iteration 68500, training loss 2.42, validation loss 2.42\n",
      "Iteration 68600, training loss 2.46, validation loss 2.42\n",
      "Iteration 68700, training loss 2.45, validation loss 2.46\n",
      "Iteration 68800, training loss 2.47, validation loss 2.44\n",
      "Iteration 68900, training loss 2.41, validation loss 2.43\n",
      "Iteration 69000, training loss 2.42, validation loss 2.42\n",
      "Iteration 69100, training loss 2.43, validation loss 2.42\n",
      "Iteration 69200, training loss 2.44, validation loss 2.42\n",
      "Iteration 69300, training loss 2.43, validation loss 2.44\n",
      "Iteration 69400, training loss 2.44, validation loss 2.42\n",
      "Iteration 69500, training loss 2.47, validation loss 2.41\n",
      "Iteration 69600, training loss 2.40, validation loss 2.40\n",
      "Iteration 69700, training loss 2.45, validation loss 2.43\n",
      "Iteration 69800, training loss 2.44, validation loss 2.44\n",
      "Iteration 69900, training loss 2.43, validation loss 2.41\n",
      "Iteration 70000, training loss 2.41, validation loss 2.41\n",
      "Iteration 70100, training loss 2.44, validation loss 2.45\n",
      "Iteration 70200, training loss 2.44, validation loss 2.40\n",
      "Iteration 70300, training loss 2.44, validation loss 2.41\n",
      "Iteration 70400, training loss 2.45, validation loss 2.42\n",
      "Iteration 70500, training loss 2.41, validation loss 2.42\n",
      "Iteration 70600, training loss 2.43, validation loss 2.44\n",
      "Iteration 70700, training loss 2.42, validation loss 2.40\n",
      "Iteration 70800, training loss 2.43, validation loss 2.42\n",
      "Iteration 70900, training loss 2.47, validation loss 2.46\n",
      "Iteration 71000, training loss 2.43, validation loss 2.40\n",
      "Iteration 71100, training loss 2.44, validation loss 2.41\n",
      "Iteration 71200, training loss 2.42, validation loss 2.43\n",
      "Iteration 71300, training loss 2.44, validation loss 2.43\n",
      "Iteration 71400, training loss 2.43, validation loss 2.40\n",
      "Iteration 71500, training loss 2.42, validation loss 2.43\n",
      "Iteration 71600, training loss 2.41, validation loss 2.43\n",
      "Iteration 71700, training loss 2.42, validation loss 2.44\n",
      "Iteration 71800, training loss 2.43, validation loss 2.43\n",
      "Iteration 71900, training loss 2.42, validation loss 2.45\n",
      "Iteration 72000, training loss 2.40, validation loss 2.42\n",
      "Iteration 72100, training loss 2.44, validation loss 2.43\n",
      "Iteration 72200, training loss 2.41, validation loss 2.42\n",
      "Iteration 72300, training loss 2.42, validation loss 2.42\n",
      "Iteration 72400, training loss 2.43, validation loss 2.42\n",
      "Iteration 72500, training loss 2.43, validation loss 2.46\n",
      "Iteration 72600, training loss 2.45, validation loss 2.40\n",
      "Iteration 72700, training loss 2.44, validation loss 2.43\n",
      "Iteration 72800, training loss 2.43, validation loss 2.45\n",
      "Iteration 72900, training loss 2.42, validation loss 2.45\n",
      "Iteration 73000, training loss 2.44, validation loss 2.45\n",
      "Iteration 73100, training loss 2.45, validation loss 2.42\n",
      "Iteration 73200, training loss 2.43, validation loss 2.43\n",
      "Iteration 73300, training loss 2.45, validation loss 2.43\n",
      "Iteration 73400, training loss 2.41, validation loss 2.41\n",
      "Iteration 73500, training loss 2.46, validation loss 2.40\n",
      "Iteration 73600, training loss 2.44, validation loss 2.44\n",
      "Iteration 73700, training loss 2.44, validation loss 2.46\n",
      "Iteration 73800, training loss 2.41, validation loss 2.44\n",
      "Iteration 73900, training loss 2.42, validation loss 2.42\n",
      "Iteration 74000, training loss 2.44, validation loss 2.40\n",
      "Iteration 74100, training loss 2.46, validation loss 2.44\n",
      "Iteration 74200, training loss 2.45, validation loss 2.39\n",
      "Iteration 74300, training loss 2.42, validation loss 2.44\n",
      "Iteration 74400, training loss 2.45, validation loss 2.43\n",
      "Iteration 74500, training loss 2.44, validation loss 2.40\n",
      "Iteration 74600, training loss 2.42, validation loss 2.41\n",
      "Iteration 74700, training loss 2.43, validation loss 2.43\n",
      "Iteration 74800, training loss 2.41, validation loss 2.42\n",
      "Iteration 74900, training loss 2.40, validation loss 2.43\n",
      "Iteration 75000, training loss 2.41, validation loss 2.43\n",
      "Iteration 75100, training loss 2.43, validation loss 2.41\n",
      "Iteration 75200, training loss 2.43, validation loss 2.44\n",
      "Iteration 75300, training loss 2.40, validation loss 2.44\n",
      "Iteration 75400, training loss 2.44, validation loss 2.45\n",
      "Iteration 75500, training loss 2.45, validation loss 2.41\n",
      "Iteration 75600, training loss 2.43, validation loss 2.43\n",
      "Iteration 75700, training loss 2.42, validation loss 2.39\n",
      "Iteration 75800, training loss 2.42, validation loss 2.42\n",
      "Iteration 75900, training loss 2.46, validation loss 2.42\n",
      "Iteration 76000, training loss 2.46, validation loss 2.43\n",
      "Iteration 76100, training loss 2.41, validation loss 2.45\n",
      "Iteration 76200, training loss 2.41, validation loss 2.44\n",
      "Iteration 76300, training loss 2.42, validation loss 2.44\n",
      "Iteration 76400, training loss 2.43, validation loss 2.41\n",
      "Iteration 76500, training loss 2.44, validation loss 2.43\n",
      "Iteration 76600, training loss 2.42, validation loss 2.45\n",
      "Iteration 76700, training loss 2.42, validation loss 2.43\n",
      "Iteration 76800, training loss 2.42, validation loss 2.44\n",
      "Iteration 76900, training loss 2.45, validation loss 2.43\n",
      "Iteration 77000, training loss 2.41, validation loss 2.41\n",
      "Iteration 77100, training loss 2.44, validation loss 2.45\n",
      "Iteration 77200, training loss 2.42, validation loss 2.44\n",
      "Iteration 77300, training loss 2.42, validation loss 2.47\n",
      "Iteration 77400, training loss 2.42, validation loss 2.44\n",
      "Iteration 77500, training loss 2.46, validation loss 2.42\n",
      "Iteration 77600, training loss 2.42, validation loss 2.43\n",
      "Iteration 77700, training loss 2.42, validation loss 2.42\n",
      "Iteration 77800, training loss 2.44, validation loss 2.43\n",
      "Iteration 77900, training loss 2.42, validation loss 2.42\n",
      "Iteration 78000, training loss 2.42, validation loss 2.43\n",
      "Iteration 78100, training loss 2.41, validation loss 2.44\n",
      "Iteration 78200, training loss 2.42, validation loss 2.42\n",
      "Iteration 78300, training loss 2.44, validation loss 2.43\n",
      "Iteration 78400, training loss 2.44, validation loss 2.44\n",
      "Iteration 78500, training loss 2.43, validation loss 2.42\n",
      "Iteration 78600, training loss 2.43, validation loss 2.44\n",
      "Iteration 78700, training loss 2.43, validation loss 2.40\n",
      "Iteration 78800, training loss 2.44, validation loss 2.44\n",
      "Iteration 78900, training loss 2.41, validation loss 2.46\n",
      "Iteration 79000, training loss 2.46, validation loss 2.43\n",
      "Iteration 79100, training loss 2.41, validation loss 2.45\n",
      "Iteration 79200, training loss 2.44, validation loss 2.44\n",
      "Iteration 79300, training loss 2.42, validation loss 2.41\n",
      "Iteration 79400, training loss 2.41, validation loss 2.44\n",
      "Iteration 79500, training loss 2.44, validation loss 2.45\n",
      "Iteration 79600, training loss 2.41, validation loss 2.43\n",
      "Iteration 79700, training loss 2.45, validation loss 2.43\n",
      "Iteration 79800, training loss 2.46, validation loss 2.41\n",
      "Iteration 79900, training loss 2.43, validation loss 2.41\n",
      "Iteration 80000, training loss 2.43, validation loss 2.39\n",
      "Iteration 80100, training loss 2.43, validation loss 2.38\n",
      "Iteration 80200, training loss 2.42, validation loss 2.41\n",
      "Iteration 80300, training loss 2.46, validation loss 2.40\n",
      "Iteration 80400, training loss 2.41, validation loss 2.45\n",
      "Iteration 80500, training loss 2.43, validation loss 2.42\n",
      "Iteration 80600, training loss 2.41, validation loss 2.42\n",
      "Iteration 80700, training loss 2.42, validation loss 2.43\n",
      "Iteration 80800, training loss 2.43, validation loss 2.44\n",
      "Iteration 80900, training loss 2.45, validation loss 2.42\n",
      "Iteration 81000, training loss 2.42, validation loss 2.42\n",
      "Iteration 81100, training loss 2.44, validation loss 2.39\n",
      "Iteration 81200, training loss 2.43, validation loss 2.41\n",
      "Iteration 81300, training loss 2.44, validation loss 2.43\n",
      "Iteration 81400, training loss 2.42, validation loss 2.42\n",
      "Iteration 81500, training loss 2.39, validation loss 2.46\n",
      "Iteration 81600, training loss 2.40, validation loss 2.43\n",
      "Iteration 81700, training loss 2.45, validation loss 2.42\n",
      "Iteration 81800, training loss 2.41, validation loss 2.43\n",
      "Iteration 81900, training loss 2.39, validation loss 2.45\n",
      "Iteration 82000, training loss 2.45, validation loss 2.43\n",
      "Iteration 82100, training loss 2.45, validation loss 2.44\n",
      "Iteration 82200, training loss 2.41, validation loss 2.43\n",
      "Iteration 82300, training loss 2.44, validation loss 2.44\n",
      "Iteration 82400, training loss 2.42, validation loss 2.39\n",
      "Iteration 82500, training loss 2.45, validation loss 2.42\n",
      "Iteration 82600, training loss 2.44, validation loss 2.42\n",
      "Iteration 82700, training loss 2.44, validation loss 2.44\n",
      "Iteration 82800, training loss 2.43, validation loss 2.45\n",
      "Iteration 82900, training loss 2.43, validation loss 2.45\n",
      "Iteration 83000, training loss 2.44, validation loss 2.40\n",
      "Iteration 83100, training loss 2.40, validation loss 2.43\n",
      "Iteration 83200, training loss 2.45, validation loss 2.42\n",
      "Iteration 83300, training loss 2.41, validation loss 2.44\n",
      "Iteration 83400, training loss 2.43, validation loss 2.44\n",
      "Iteration 83500, training loss 2.40, validation loss 2.43\n",
      "Iteration 83600, training loss 2.41, validation loss 2.44\n",
      "Iteration 83700, training loss 2.39, validation loss 2.44\n",
      "Iteration 83800, training loss 2.43, validation loss 2.41\n",
      "Iteration 83900, training loss 2.42, validation loss 2.42\n",
      "Iteration 84000, training loss 2.43, validation loss 2.44\n",
      "Iteration 84100, training loss 2.42, validation loss 2.40\n",
      "Iteration 84200, training loss 2.42, validation loss 2.44\n",
      "Iteration 84300, training loss 2.45, validation loss 2.43\n",
      "Iteration 84400, training loss 2.42, validation loss 2.43\n",
      "Iteration 84500, training loss 2.45, validation loss 2.46\n",
      "Iteration 84600, training loss 2.45, validation loss 2.41\n",
      "Iteration 84700, training loss 2.42, validation loss 2.43\n",
      "Iteration 84800, training loss 2.39, validation loss 2.43\n",
      "Iteration 84900, training loss 2.43, validation loss 2.41\n",
      "Iteration 85000, training loss 2.44, validation loss 2.44\n",
      "Iteration 85100, training loss 2.44, validation loss 2.41\n",
      "Iteration 85200, training loss 2.42, validation loss 2.43\n",
      "Iteration 85300, training loss 2.43, validation loss 2.44\n",
      "Iteration 85400, training loss 2.44, validation loss 2.41\n",
      "Iteration 85500, training loss 2.42, validation loss 2.47\n",
      "Iteration 85600, training loss 2.39, validation loss 2.40\n",
      "Iteration 85700, training loss 2.40, validation loss 2.43\n",
      "Iteration 85800, training loss 2.47, validation loss 2.43\n",
      "Iteration 85900, training loss 2.46, validation loss 2.43\n",
      "Iteration 86000, training loss 2.44, validation loss 2.46\n",
      "Iteration 86100, training loss 2.41, validation loss 2.41\n",
      "Iteration 86200, training loss 2.47, validation loss 2.44\n",
      "Iteration 86300, training loss 2.43, validation loss 2.44\n",
      "Iteration 86400, training loss 2.43, validation loss 2.42\n",
      "Iteration 86500, training loss 2.43, validation loss 2.40\n",
      "Iteration 86600, training loss 2.41, validation loss 2.43\n",
      "Iteration 86700, training loss 2.42, validation loss 2.41\n",
      "Iteration 86800, training loss 2.43, validation loss 2.44\n",
      "Iteration 86900, training loss 2.42, validation loss 2.44\n",
      "Iteration 87000, training loss 2.42, validation loss 2.46\n",
      "Iteration 87100, training loss 2.42, validation loss 2.46\n",
      "Iteration 87200, training loss 2.43, validation loss 2.43\n",
      "Iteration 87300, training loss 2.44, validation loss 2.42\n",
      "Iteration 87400, training loss 2.43, validation loss 2.43\n",
      "Iteration 87500, training loss 2.46, validation loss 2.41\n",
      "Iteration 87600, training loss 2.41, validation loss 2.39\n",
      "Iteration 87700, training loss 2.43, validation loss 2.43\n",
      "Iteration 87800, training loss 2.42, validation loss 2.46\n",
      "Iteration 87900, training loss 2.40, validation loss 2.38\n",
      "Iteration 88000, training loss 2.42, validation loss 2.39\n",
      "Iteration 88100, training loss 2.41, validation loss 2.43\n",
      "Iteration 88200, training loss 2.41, validation loss 2.41\n",
      "Iteration 88300, training loss 2.43, validation loss 2.44\n",
      "Iteration 88400, training loss 2.43, validation loss 2.40\n",
      "Iteration 88500, training loss 2.41, validation loss 2.42\n",
      "Iteration 88600, training loss 2.40, validation loss 2.45\n",
      "Iteration 88700, training loss 2.43, validation loss 2.43\n",
      "Iteration 88800, training loss 2.43, validation loss 2.41\n",
      "Iteration 88900, training loss 2.41, validation loss 2.45\n",
      "Iteration 89000, training loss 2.44, validation loss 2.43\n",
      "Iteration 89100, training loss 2.43, validation loss 2.42\n",
      "Iteration 89200, training loss 2.43, validation loss 2.40\n",
      "Iteration 89300, training loss 2.44, validation loss 2.40\n",
      "Iteration 89400, training loss 2.42, validation loss 2.41\n",
      "Iteration 89500, training loss 2.42, validation loss 2.42\n",
      "Iteration 89600, training loss 2.44, validation loss 2.43\n",
      "Iteration 89700, training loss 2.44, validation loss 2.45\n",
      "Iteration 89800, training loss 2.44, validation loss 2.46\n",
      "Iteration 89900, training loss 2.45, validation loss 2.43\n",
      "Iteration 90000, training loss 2.42, validation loss 2.42\n",
      "Iteration 90100, training loss 2.44, validation loss 2.43\n",
      "Iteration 90200, training loss 2.46, validation loss 2.46\n",
      "Iteration 90300, training loss 2.45, validation loss 2.42\n",
      "Iteration 90400, training loss 2.44, validation loss 2.44\n",
      "Iteration 90500, training loss 2.42, validation loss 2.44\n",
      "Iteration 90600, training loss 2.43, validation loss 2.44\n",
      "Iteration 90700, training loss 2.42, validation loss 2.37\n",
      "Iteration 90800, training loss 2.40, validation loss 2.44\n",
      "Iteration 90900, training loss 2.43, validation loss 2.43\n",
      "Iteration 91000, training loss 2.44, validation loss 2.41\n",
      "Iteration 91100, training loss 2.45, validation loss 2.45\n",
      "Iteration 91200, training loss 2.42, validation loss 2.45\n",
      "Iteration 91300, training loss 2.42, validation loss 2.42\n",
      "Iteration 91400, training loss 2.41, validation loss 2.44\n",
      "Iteration 91500, training loss 2.44, validation loss 2.44\n",
      "Iteration 91600, training loss 2.47, validation loss 2.44\n",
      "Iteration 91700, training loss 2.42, validation loss 2.43\n",
      "Iteration 91800, training loss 2.43, validation loss 2.44\n",
      "Iteration 91900, training loss 2.44, validation loss 2.42\n",
      "Iteration 92000, training loss 2.43, validation loss 2.43\n",
      "Iteration 92100, training loss 2.41, validation loss 2.43\n",
      "Iteration 92200, training loss 2.42, validation loss 2.42\n",
      "Iteration 92300, training loss 2.43, validation loss 2.45\n",
      "Iteration 92400, training loss 2.43, validation loss 2.46\n",
      "Iteration 92500, training loss 2.40, validation loss 2.45\n",
      "Iteration 92600, training loss 2.43, validation loss 2.41\n",
      "Iteration 92700, training loss 2.45, validation loss 2.44\n",
      "Iteration 92800, training loss 2.42, validation loss 2.42\n",
      "Iteration 92900, training loss 2.43, validation loss 2.45\n",
      "Iteration 93000, training loss 2.42, validation loss 2.42\n",
      "Iteration 93100, training loss 2.44, validation loss 2.40\n",
      "Iteration 93200, training loss 2.41, validation loss 2.43\n",
      "Iteration 93300, training loss 2.43, validation loss 2.43\n",
      "Iteration 93400, training loss 2.41, validation loss 2.42\n",
      "Iteration 93500, training loss 2.43, validation loss 2.38\n",
      "Iteration 93600, training loss 2.45, validation loss 2.43\n",
      "Iteration 93700, training loss 2.45, validation loss 2.43\n",
      "Iteration 93800, training loss 2.45, validation loss 2.41\n",
      "Iteration 93900, training loss 2.41, validation loss 2.45\n",
      "Iteration 94000, training loss 2.41, validation loss 2.42\n",
      "Iteration 94100, training loss 2.45, validation loss 2.42\n",
      "Iteration 94200, training loss 2.41, validation loss 2.44\n",
      "Iteration 94300, training loss 2.43, validation loss 2.44\n",
      "Iteration 94400, training loss 2.42, validation loss 2.43\n",
      "Iteration 94500, training loss 2.44, validation loss 2.44\n",
      "Iteration 94600, training loss 2.40, validation loss 2.38\n",
      "Iteration 94700, training loss 2.43, validation loss 2.44\n",
      "Iteration 94800, training loss 2.44, validation loss 2.42\n",
      "Iteration 94900, training loss 2.42, validation loss 2.41\n",
      "Iteration 95000, training loss 2.44, validation loss 2.44\n",
      "Iteration 95100, training loss 2.41, validation loss 2.42\n",
      "Iteration 95200, training loss 2.43, validation loss 2.44\n",
      "Iteration 95300, training loss 2.45, validation loss 2.44\n",
      "Iteration 95400, training loss 2.37, validation loss 2.43\n",
      "Iteration 95500, training loss 2.45, validation loss 2.41\n",
      "Iteration 95600, training loss 2.45, validation loss 2.42\n",
      "Iteration 95700, training loss 2.42, validation loss 2.43\n",
      "Iteration 95800, training loss 2.44, validation loss 2.44\n",
      "Iteration 95900, training loss 2.44, validation loss 2.43\n",
      "Iteration 96000, training loss 2.44, validation loss 2.43\n",
      "Iteration 96100, training loss 2.42, validation loss 2.43\n",
      "Iteration 96200, training loss 2.45, validation loss 2.46\n",
      "Iteration 96300, training loss 2.44, validation loss 2.42\n",
      "Iteration 96400, training loss 2.44, validation loss 2.45\n",
      "Iteration 96500, training loss 2.43, validation loss 2.44\n",
      "Iteration 96600, training loss 2.42, validation loss 2.43\n",
      "Iteration 96700, training loss 2.42, validation loss 2.43\n",
      "Iteration 96800, training loss 2.44, validation loss 2.41\n",
      "Iteration 96900, training loss 2.42, validation loss 2.41\n",
      "Iteration 97000, training loss 2.45, validation loss 2.42\n",
      "Iteration 97100, training loss 2.45, validation loss 2.46\n",
      "Iteration 97200, training loss 2.44, validation loss 2.41\n",
      "Iteration 97300, training loss 2.42, validation loss 2.41\n",
      "Iteration 97400, training loss 2.43, validation loss 2.44\n",
      "Iteration 97500, training loss 2.48, validation loss 2.41\n",
      "Iteration 97600, training loss 2.41, validation loss 2.44\n",
      "Iteration 97700, training loss 2.42, validation loss 2.41\n",
      "Iteration 97800, training loss 2.45, validation loss 2.43\n",
      "Iteration 97900, training loss 2.40, validation loss 2.42\n",
      "Iteration 98000, training loss 2.45, validation loss 2.45\n",
      "Iteration 98100, training loss 2.46, validation loss 2.44\n",
      "Iteration 98200, training loss 2.45, validation loss 2.43\n",
      "Iteration 98300, training loss 2.42, validation loss 2.44\n",
      "Iteration 98400, training loss 2.42, validation loss 2.44\n",
      "Iteration 98500, training loss 2.44, validation loss 2.44\n",
      "Iteration 98600, training loss 2.42, validation loss 2.44\n",
      "Iteration 98700, training loss 2.44, validation loss 2.43\n",
      "Iteration 98800, training loss 2.42, validation loss 2.45\n",
      "Iteration 98900, training loss 2.45, validation loss 2.45\n",
      "Iteration 99000, training loss 2.42, validation loss 2.46\n",
      "Iteration 99100, training loss 2.44, validation loss 2.44\n",
      "Iteration 99200, training loss 2.43, validation loss 2.44\n",
      "Iteration 99300, training loss 2.44, validation loss 2.41\n",
      "Iteration 99400, training loss 2.41, validation loss 2.42\n",
      "Iteration 99500, training loss 2.44, validation loss 2.45\n",
      "Iteration 99600, training loss 2.42, validation loss 2.41\n",
      "Iteration 99700, training loss 2.41, validation loss 2.40\n",
      "Iteration 99800, training loss 2.40, validation loss 2.43\n",
      "Iteration 99900, training loss 2.44, validation loss 2.42\n",
      "2.4414849281311035\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 100000\n",
    "learning_rate = 3e-8\n",
    "# 3e-4 is around 0.0003\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iterations):\n",
    "    if iter % 100 == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Iteration {iter}, training loss {losses['train']:.2f}, validation loss {losses['val']:.2f}\")\n",
    "    inputs, targets = get_batch('train')\n",
    "    logits, loss = model.forward_pass(inputs, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f361d452-ddf7-4ecf-95ba-92fa389d11ad",
   "metadata": {},
   "source": [
    "## 9) test generation after the very basic training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d59fc18-9cad-49dc-be47-af43c598d26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 58, 51, 44, 1, 29, 46, 51, 44]\n",
      "\n",
      "ROMy lo t, njULTIENUT&æy, nd wis imerdst pod se I foo frad thing nd CEn thtpillarewe adereg wigep e is Roco geman ay on ochey RI nn thay ho t bove.\n",
      "\n",
      "\n",
      "Jard, ncakefartch lt deame, bulve in.\n",
      "\n",
      "A h, Pucal yo ire id avee ds t hat ithon as blim.\n",
      "PSthind we meck the, wa dis thetourtothoupholsenlll lkite thathe wes ithel che,\n",
      "JULI meaworeea Ca e orot.\n",
      "For int!\n",
      "HVe,\n",
      "\n",
      "\n",
      "Amye t a thislar ofrmer bl\n",
      " OMETIO. ed an, ay pour hy, pu hin thermer s thotwe t therckn, f certokis wne mandruleyosus, me.\n",
      "Anefifeng, bome\n"
     ]
    }
   ],
   "source": [
    "test_input = encoder('Yung Ting')\n",
    "print(test_input)\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_tokens = m.generate_tokens(context, max_new_tokens=500)\n",
    "generated_chars = decoder(generated_tokens[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bff1b18-6920-44ea-8037-dab02cf10f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4631a-7a2b-4016-87e2-53a77b0cdcca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yungting-gpt",
   "language": "python",
   "name": "yugpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
