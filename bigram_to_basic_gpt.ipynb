{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "492ffea2-09f9-41e1-992a-57691593bd88",
   "metadata": {},
   "source": [
    "## The transformer architecture\n",
    "\n",
    "<div>\n",
    "<img src=\"https://heidloff.net/assets/img/2023/02/transformers.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "- #### how does a generative pre-trained transformer differ from a transformer?\n",
    "    - a gpt has no encoder and multi-head attention (what the encoder plugs into)\n",
    "    - so it will basically :\n",
    "        - i) masked multi head attention\n",
    "        - ii) add and normalise\n",
    "        - iii) feed forward\n",
    "        - iv) add and normalise\n",
    "        - v) linear transformation\n",
    "        - vi) softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438136a-04ca-47fe-a0d5-b0056d9d1604",
   "metadata": {},
   "source": [
    "# Converting the Romeo and Juliet model to a GPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a33de4-5faf-47e4-a688-a3100756a9fe",
   "metadata": {},
   "source": [
    "## 1) import necessary modules and set the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116f329e-f916-41f8-b224-4327e9dd3cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "block_size = 64\n",
    "batch_size = 128\n",
    "max_iterations = 5000\n",
    "learning_rate = 3e-4\n",
    "evaluation_iterations = 100\n",
    "evaluation_interval = 200\n",
    "n_embd = 384\n",
    "n_head = 8\n",
    "n_layer = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d13cec-8822-481a-b4f3-330a0685e382",
   "metadata": {},
   "source": [
    "## 2) read the text file with data and make a sorted set of characters to get the vocab_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2264cef-6173-4b84-b969-cd55d70746c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()  \n",
    "    \n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacedd71-2c4e-491b-a23c-c07359dbdc77",
   "metadata": {},
   "source": [
    "## 3) make a character-level tokenizer and encode the text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb074b20-e5dc-4925-9a9b-450a989f6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_integer = { ch:i for i,ch in enumerate(chars) }\n",
    "integer_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encoder = lambda s: [string_to_integer[c] for c in s]\n",
    "decoder = lambda l: ''.join([integer_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encoder(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e71750-3ea8-4312-8997-87ecd675269f",
   "metadata": {},
   "source": [
    "## 4) Create training and Validation splits and define the get_batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235ba6f5-9d98-4152-92dd-0a4ab9a3b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = int(0.8*len(data))\n",
    "training_data = data[:split_size]\n",
    "validation_data = data[split_size:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = training_data if split == 'train' else validation_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cba0fa-3dd9-48b4-81df-7544a6eb44ea",
   "metadata": {},
   "source": [
    "## 5) Define the estimate loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab88114-7d8f-4ed7-8b63-66863778b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(evaluation_iterations)\n",
    "        for k in range(evaluation_iterations):\n",
    "            inputs, targets = get_batch(split)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d827bf-aef9-45d2-b9bf-f68467cceea8",
   "metadata": {},
   "source": [
    "## 6) Define the gpt model class and initialise a model\n",
    "- #### i) we add positional encoding as per the architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a866531-a5ac-4ca8-9d79-e0bf75e14366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\"\n",
    "        Initializes the self-attention head.\n",
    "        \n",
    "        Args:\n",
    "            head_size (int): The dimensionality of each attention head.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Linear layers to project input embeddings to key, query, and value vectors\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        # Create a lower triangular matrix for causal masking to prevent attention to future tokens\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the self-attention head.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, T, C), where\n",
    "                              B = Batch size,\n",
    "                              T = Sequence length,\n",
    "                              C = Embedding dimension.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying self-attention, shape (B, T, head_size).\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape  # Unpack the input shape\n",
    "        \n",
    "        # Project input embeddings to keys and queries\n",
    "        k = self.key(x)    # Shape: (B, T, head_size)\n",
    "        q = self.query(x)  # Shape: (B, T, head_size)\n",
    "        \n",
    "        # Compute attention scores by taking the dot product of queries and keys\n",
    "        # Transpose k to shape (B, head_size, T) for batch matrix multiplication\n",
    "        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)  # Shape: (B, T, T)\n",
    "        \n",
    "        # Apply causal masking to ensure each position can only attend to previous positions\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Shape: (B, T, T)\n",
    "        \n",
    "        # Apply softmax to obtain attention weights\n",
    "        wei = F.softmax(wei, dim=-1)  # Shape: (B, T, T)\n",
    "        \n",
    "        # Apply dropout to the attention weights for regularization\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Project input embeddings to values\n",
    "        v = self.value(x)  # Shape: (B, T, head_size)\n",
    "        \n",
    "        # Perform weighted aggregation of the values based on attention weights\n",
    "        out = wei @ v  # Shape: (B, T, head_size)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        \"\"\"\n",
    "        Initializes the multi-head self-attention module.\n",
    "        \n",
    "        Args:\n",
    "            num_heads (int): Number of attention heads.\n",
    "            head_size (int): Dimensionality of each attention head.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Create a list of Head modules\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "        # Linear layer to project concatenated head outputs back to embedding dimension\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for multi-head self-attention.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, T, C).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after multi-head attention, shape (B, T, n_embd).\n",
    "        \"\"\"\n",
    "        # Concatenate outputs from all attention heads along the embedding dimension\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # Shape: (B, T, head_size * num_heads)\n",
    "        \n",
    "        # Project the concatenated outputs back to the original embedding dimension\n",
    "        out = self.dropout(self.proj(out))  # Shape: (B, T, n_embd)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple linear layer followed by a non-linearity.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        \"\"\"\n",
    "        Initializes the feedforward network.\n",
    "        \n",
    "        Args:\n",
    "            n_embd (int): Embedding dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Define a sequential network comprising linear layers and ReLU activation\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # Expand embedding dimension\n",
    "            nn.ReLU(),                      # Apply ReLU activation\n",
    "            nn.Linear(4 * n_embd, n_embd),  # Project back to original embedding dimension\n",
    "            nn.Dropout(dropout),            # Apply dropout for regularization\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the feedforward network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, T, C).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after feedforward processing, shape (B, T, C).\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer block.\n",
    "        \n",
    "        Args:\n",
    "            n_embd (int): Embedding dimension.\n",
    "            n_head (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head  # Determine head size based on embedding dimension and number of heads\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # Multi-head self-attention module\n",
    "        self.ffwd = FeedForward(n_embd)                  # Feedforward network\n",
    "        self.ln1 = nn.LayerNorm(n_embd)                  # Layer normalization after attention\n",
    "        self.ln2 = nn.LayerNorm(n_embd)                  # Layer normalization after feedforward\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the Transformer block.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, T, C).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after processing, shape (B, T, C).\n",
    "        \"\"\"\n",
    "        # Apply multi-head self-attention\n",
    "        y = self.sa(x)  # Shape: (B, T, C)\n",
    "        \n",
    "        # Add residual connection and apply layer normalization\n",
    "        x = self.ln1(x + y)  # Shape: (B, T, C)\n",
    "        \n",
    "        # Apply feedforward network\n",
    "        y = self.ffwd(x)  # Shape: (B, T, C)\n",
    "        \n",
    "        # Add residual connection and apply layer normalization\n",
    "        x = self.ln2(x + y)  # Shape: (B, T, C)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"GPT Language Model implementing Transformer architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        Initializes the GPT language model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary (number of unique tokens).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Token embedding table maps each token index to an embedding vector\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        \n",
    "        # Position embedding table provides positional information for each token in the sequence\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        # Stack multiple Transformer blocks to build the model's depth\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        \n",
    "        # Final layer normalization for stabilizing the output\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # Language modeling head projects the final embeddings to vocabulary logits\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        # Initialize weights using the defined method\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initializes weights of the model's layers.\n",
    "        \n",
    "        Args:\n",
    "            module (nn.Module): A module within the model.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Initialize linear layers with normal distribution (mean=0, std=0.02)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                # Initialize biases to zero\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # Initialize embedding layers with normal distribution (mean=0, std=0.02)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, index, targets=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the GPT language model.\n",
    "        \n",
    "        Args:\n",
    "            index (torch.Tensor): Input tensor of token indices, shape (B, T).\n",
    "            targets (torch.Tensor, optional): Target tensor of token indices, shape (B, T).\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "                - logits: Predicted logits for each token, shape (B, T, vocab_size).\n",
    "                - loss: Cross-entropy loss (if targets are provided), else None.\n",
    "        \"\"\"\n",
    "        B, T = index.shape  # Unpack batch size and sequence length\n",
    "        \n",
    "        # Retrieve token embeddings for the input indices\n",
    "        tok_emb = self.token_embedding_table(index)  # Shape: (B, T, C)\n",
    "        \n",
    "        # Create a range of positions and retrieve their embeddings\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # Shape: (T, C)\n",
    "        \n",
    "        # Add token and position embeddings to incorporate positional information\n",
    "        x = tok_emb + pos_emb  # Shape: (B, T, C)\n",
    "        \n",
    "        # Pass the embeddings through the stack of Transformer blocks\n",
    "        x = self.blocks(x)  # Shape: (B, T, C)\n",
    "        \n",
    "        # Apply final layer normalization\n",
    "        x = self.ln_f(x)  # Shape: (B, T, C)\n",
    "        \n",
    "        # Project the normalized embeddings to vocabulary logits\n",
    "        logits = self.lm_head(x)  # Shape: (B, T, vocab_size)\n",
    "        \n",
    "        # If targets are provided, compute the cross-entropy loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshape logits and targets for loss computation\n",
    "            logits = logits.view(B * T, -1)    # Shape: (B*T, vocab_size)\n",
    "            targets = targets.view(B * T)      # Shape: (B*T)\n",
    "            loss = F.cross_entropy(logits, targets)  # Scalar loss value\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generates new tokens based on the input context.\n",
    "        \n",
    "        Args:\n",
    "            index (torch.Tensor): Input tensor of token indices, shape (B, T).\n",
    "            max_new_tokens (int): Number of new tokens to generate.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Generated token indices, shape (B, T + max_new_tokens).\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop the input indices to the last block_size tokens to adhere to the model's maximum context length\n",
    "            index_cond = index[:, -block_size:]\n",
    "            \n",
    "            # Perform a forward pass to get the logits for the current context\n",
    "            logits, _ = self.forward(index_cond)\n",
    "            \n",
    "            # Focus on the logits of the last time step (the most recent token)\n",
    "            logits = logits[:, -1, :]  # Shape: (B, C)\n",
    "            \n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # Shape: (B, C)\n",
    "            \n",
    "            # Sample the next token index from the probability distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1)  # Shape: (B, 1)\n",
    "            \n",
    "            # Append the sampled token to the sequence\n",
    "            index = torch.cat((index, index_next), dim=1)  # Shape: (B, T + 1)\n",
    "        \n",
    "        return index\n",
    "\n",
    "# Instantiate the GPT language model with the specified vocabulary size\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "# Move the model to the specified device (CPU or GPU)\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08712a-8d3e-49fb-9b4f-5537a3df283b",
   "metadata": {},
   "source": [
    "## 7) create an AdamW optimiser and define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d14b044-ac91-47ae-9d92-ed45a45ff004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss 4.38, validation loss 4.39\n",
      "Iteration 100, training loss 2.34, validation loss 2.40\n",
      "Iteration 200, training loss 1.97, validation loss 2.08\n",
      "Iteration 300, training loss 1.77, validation loss 1.89\n",
      "Iteration 400, training loss 1.63, validation loss 1.79\n",
      "Iteration 500, training loss 1.54, validation loss 1.73\n",
      "Iteration 600, training loss 1.47, validation loss 1.69\n",
      "Iteration 700, training loss 1.41, validation loss 1.66\n",
      "Iteration 800, training loss 1.35, validation loss 1.64\n",
      "Iteration 900, training loss 1.30, validation loss 1.65\n",
      "Iteration 1000, training loss 1.25, validation loss 1.65\n",
      "Iteration 1100, training loss 1.20, validation loss 1.65\n",
      "Iteration 1200, training loss 1.14, validation loss 1.64\n",
      "Iteration 1300, training loss 1.09, validation loss 1.66\n",
      "Iteration 1400, training loss 1.05, validation loss 1.67\n",
      "Iteration 1500, training loss 1.00, validation loss 1.69\n",
      "Iteration 1600, training loss 0.95, validation loss 1.72\n",
      "Iteration 1700, training loss 0.90, validation loss 1.73\n",
      "Iteration 1800, training loss 0.85, validation loss 1.77\n",
      "Iteration 1900, training loss 0.80, validation loss 1.81\n",
      "Iteration 2000, training loss 0.76, validation loss 1.83\n",
      "Iteration 2100, training loss 0.71, validation loss 1.88\n",
      "Iteration 2200, training loss 0.67, validation loss 1.90\n",
      "Iteration 2300, training loss 0.63, validation loss 1.94\n",
      "Iteration 2400, training loss 0.59, validation loss 2.00\n",
      "Iteration 2500, training loss 0.55, validation loss 2.02\n",
      "Iteration 2600, training loss 0.52, validation loss 2.07\n",
      "Iteration 2700, training loss 0.49, validation loss 2.12\n",
      "Iteration 2800, training loss 0.46, validation loss 2.16\n",
      "Iteration 2900, training loss 0.43, validation loss 2.19\n",
      "Iteration 3000, training loss 0.40, validation loss 2.23\n",
      "Iteration 3100, training loss 0.38, validation loss 2.26\n",
      "Iteration 3200, training loss 0.36, validation loss 2.31\n",
      "Iteration 3300, training loss 0.34, validation loss 2.36\n",
      "Iteration 3400, training loss 0.32, validation loss 2.40\n",
      "Iteration 3500, training loss 0.31, validation loss 2.46\n",
      "Iteration 3600, training loss 0.30, validation loss 2.45\n",
      "Iteration 3700, training loss 0.28, validation loss 2.52\n",
      "Iteration 3800, training loss 0.28, validation loss 2.57\n",
      "Iteration 3900, training loss 0.26, validation loss 2.60\n",
      "Iteration 4000, training loss 0.26, validation loss 2.63\n",
      "Iteration 4100, training loss 0.25, validation loss 2.68\n",
      "Iteration 4200, training loss 0.25, validation loss 2.67\n",
      "Iteration 4300, training loss 0.24, validation loss 2.72\n",
      "Iteration 4400, training loss 0.23, validation loss 2.77\n",
      "Iteration 4500, training loss 0.23, validation loss 2.81\n",
      "Iteration 4600, training loss 0.23, validation loss 2.84\n",
      "Iteration 4700, training loss 0.22, validation loss 2.86\n",
      "Iteration 4800, training loss 0.22, validation loss 2.89\n",
      "Iteration 4900, training loss 0.22, validation loss 2.90\n",
      "0.4126903712749481\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iterations):\n",
    "    if iter % evaluation_iterations == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Iteration {iter}, training loss {losses['train']:.2f}, validation loss {losses['val']:.2f}\")\n",
    "    inputs, targets = get_batch('train')\n",
    "    logits, loss = model.forward(inputs, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "849cb7d9-501d-4279-951e-0ea316e50c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi i am yung tings bers in cold make him list,\n",
      "Enor kinsman is old, what passesâ€™d it shall.\n",
      "In tell you go to thee ni\n"
     ]
    }
   ],
   "source": [
    "test_prompt = 'hi i am yung ting'\n",
    "context = torch.tensor(encoder(test_prompt), dtype=torch.long, device=device)\n",
    "# context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_tokens = m.generate(context.unsqueeze(0), max_new_tokens=100)\n",
    "generated_chars = decoder(generated_tokens[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4df86-d99f-4cc6-b6e0-340160fdbcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49314257-8187-4d8d-bf6d-a2de7590e843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yungting-gpt",
   "language": "python",
   "name": "yugpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
