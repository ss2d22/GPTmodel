{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "492ffea2-09f9-41e1-992a-57691593bd88",
   "metadata": {},
   "source": [
    "## The transformer architecture for yung ting\n",
    "\n",
    "<div>\n",
    "<img src=\"https://heidloff.net/assets/img/2023/02/transformers.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "- #### how does a generative pre-trained transformer differ from a transformer?\n",
    "    - a gpt has no encoder and multi-head attention (what the encoder plugs into)\n",
    "    - so it will basically :\n",
    "        - i) masked multi head attention\n",
    "        - ii) add and normalise\n",
    "        - iii) feed forward\n",
    "        - iv) add and normalise\n",
    "        - v) linear transformation\n",
    "        - vi) softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438136a-04ca-47fe-a0d5-b0056d9d1604",
   "metadata": {},
   "source": [
    "# Converting the Romeo and Juliet model to a GPT model for yung ting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a33de4-5faf-47e4-a688-a3100756a9fe",
   "metadata": {},
   "source": [
    "## 1) import necessary modules and set the hyperparameters\n",
    "\n",
    "- same as before but we define the hyper parameters up here as per best practices\n",
    "- what each hyper parameter is for :\n",
    "    - `device` : device we are using i.e `cuda`(nvidia gpu) or `mps` (metal accelaration on mac) or `cpu` \n",
    "    - `block_size` : size of each batch (you can think of this as the size of each brick and block size as how many bricks stacked)\n",
    "    - `batch_size` : amount of batches (how many stacked)\n",
    "    - `max_iterations` : amount of iterations for the training loop (adjust as needed)\n",
    "    - `learning_rate` : learning rate for our model (too low or too high is bad, experiment around to find what work)\n",
    "    - `evaluation_iterations` : the amount of interations to wait before evaluating the model to calculate training loss\n",
    "    - `n_embd` : embedding dimension \n",
    "    - `n_head` : number of heads\n",
    "    - `n_layer` : number of layers\n",
    "    - `dropout` : dropout rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116f329e-f916-41f8-b224-4327e9dd3cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "block_size = 64\n",
    "batch_size = 128\n",
    "max_iterations = 5000\n",
    "learning_rate = 3e-4\n",
    "evaluation_iterations = 100\n",
    "evaluation_interval = 200\n",
    "n_embd = 384\n",
    "n_head = 8\n",
    "n_layer = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d13cec-8822-481a-b4f3-330a0685e382",
   "metadata": {},
   "source": [
    "## 2) read the text file with data and make a sorted set of characters to get the vocab_size \n",
    "\n",
    "- same as before (if needed refer to the bigram model file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2264cef-6173-4b84-b969-cd55d70746c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()  \n",
    "    \n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacedd71-2c4e-491b-a23c-c07359dbdc77",
   "metadata": {},
   "source": [
    "## 3) make a character-level tokenizer and encode the text corpus\n",
    "\n",
    "- same as before (if needed refer to the bigram model file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb074b20-e5dc-4925-9a9b-450a989f6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_integer = { ch:i for i,ch in enumerate(chars) }\n",
    "integer_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encoder = lambda s: [string_to_integer[c] for c in s]\n",
    "decoder = lambda l: ''.join([integer_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encoder(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e71750-3ea8-4312-8997-87ecd675269f",
   "metadata": {},
   "source": [
    "## 4) Create training and Validation splits and define the get_batch function\n",
    "\n",
    "- same as before (if needed refer to the bigram model file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235ba6f5-9d98-4152-92dd-0a4ab9a3b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = int(0.8*len(data))\n",
    "training_data = data[:split_size]\n",
    "validation_data = data[split_size:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = training_data if split == 'train' else validation_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cba0fa-3dd9-48b4-81df-7544a6eb44ea",
   "metadata": {},
   "source": [
    "## 5) Define the estimate loss function\n",
    "\n",
    "- same as before (if needed refer to the bigram model file)\n",
    "- only difference being us using the hyperparameter properly here , it was hard coded in the old bigram model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab88114-7d8f-4ed7-8b63-66863778b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(evaluation_iterations)\n",
    "        for k in range(evaluation_iterations):\n",
    "            inputs, targets = get_batch(split)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d827bf-aef9-45d2-b9bf-f68467cceea8",
   "metadata": {},
   "source": [
    "## 6) Define the GPT Model class, Transformer Block class, Feed Forward Class, Multiple Head attention class and Head class and initialise a model and load it into our device \n",
    "\n",
    "- #### i) Changes to the GPTLanguageModel class initialisation (used to be called BigramModel class for bigram model so yah):\n",
    "    - ` self.token_embedding_table = nn.Embedding(vocab_size, n_embd)` : we change `vocab_size x vocab_size` to `vocab_size x n_embd` making the embedding table a lot larger \n",
    "    - `self.position_embedding_table = nn.Embedding(block_size, n_embd)` : we add positional embedding as per the architechture\n",
    "        - Position embedding table provides positional information for each token in the sequence\n",
    "    - `self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])` : we stack multiple transformer blocks on top of each other to build the required depth to our model.\n",
    "        - we create a transformer block with the size of the embedding table and the number of heads\n",
    "        - we repeat the step above for `n_layer` times \n",
    "        - so if `n_head` is 5 and `n_embd` is 100 , and `n_layer` is 6 we create `Block(100, n_head=6)` 6 times\n",
    "        - finally we stack that all\n",
    "    - `self.ln_f = nn.LayerNorm(n_embd)` : we create a layer normalisation function with the size of our kayer (horizontal embedding table length)\n",
    "        - this is for stabilizing the outputs \n",
    "    - `self.lm_head = nn.Linear(n_embd, vocab_size)` : Linear transformation to project the final embeddings to vocabulary logits (if you do not remember what logits are refer to the bigram file)\n",
    "    - ` self.apply(self._init_weights)` : use the defined function to initialise weights\n",
    "\n",
    "- #### ii) defining the init_weights function for the gpt model class \n",
    "    - the `init_weight` function which accepts a module from the class\n",
    "        - `if isinstance(module, nn.Linear)` : checks if the passed in instance is part of `nn.linear` then :\n",
    "            - `torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)` : initialises the module with normal distrubition with a mean of 0 and standard distribution of 0.02\n",
    "            - `if module.bias is not None` : checks if there are any existig biases , if there are any existing biases then :\n",
    "                - `torch.nn.init.zeros_(module.bias)` : initialises the biases to 0\n",
    "            - `elif isinstance(module, nn.Embedding)` : checks if the passed in module is of `nn.embedding` if i is:\n",
    "                - `torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)` : initialises the weights with mean of 0 and standard deviation of 0.02\n",
    "                    - we do not check for biases here and that is the only difference here , we do not check as tjis is part of `nn.embedding`\n",
    "\n",
    "- #### iii) updating the forward pass function to follow the GPT architecture \n",
    "    - we accept `index` and optionally `targets` same as before \n",
    "    - ` B, T = index.shape` : since our index tensor will have a shape of (B, T) we will have to reshape it before using it for certain operations in the function hence we will first unpack the tensor\n",
    "    - `tok_emb = self.token_embedding_table(index)` : we retrieve the embeddings for the given indices\n",
    "    - `self.position_embedding_table = nn.Embedding(block_size, n_embd)` : we create a range of positions and retrieve their embeddings\n",
    "    - `x = tok_emb + pos_emb ` : we add the token and positional embedding information together to add positional information to the token emneddings\n",
    "    - `x = self.blocks(x)` : we pass the embeddings through the transformer blocks\n",
    "    - `x = self.ln_f(x)` : we apply the final layer normalisation to stabilise the outputs\n",
    "    - `logits = self.lm_head(x)` : we project the normalized embeddings to vocabulary logits\n",
    "    - `if targets is None` : we check if the optional targets is not given (not training):\n",
    "        - `loss = None` : we just set the loss to none as loss calculation is not needed\n",
    "    - else we :\n",
    "        - below is repeated from the other file\n",
    "        - `logits = logits.view(B * T, -1)` : reshape out logits for loss computation\n",
    "        - `targets = targets.view(B * T) ` : reshape our targets for loss computation \n",
    "        - we do the above reshaping due to the shapes required by the cross entropy function\n",
    "        - `loss = F.cross_entropy(logits, targets)` : we calculate the scalar loss value using the cross entorpy fiction\n",
    "    - `return logits, loss` : finally we return the logits and the loss (can be None)\n",
    "\n",
    "- #### iv) updating the generate tokens function to follow the GPT architecture\n",
    "    - we accept `index` and `max_new_tokens` as parameters same as before\n",
    "    - `for _ in range(max_new_tokens)` : we have a loop that runs for the amount of new tokens we wanna generate\n",
    "        - `index_cond = index[:, -block_size:]` : we crop the input indices to the last block_size tokens to adhere to the model's maximum context length\n",
    "        - `logits, _ = self.forward(index_cond)` : we pass the current context into the forward pass to get the logits for the current context\n",
    "        - `logits = logits[:, -1, :] ` : we focus on the logits of the last time step (the most recent token), which is why we do -1\n",
    "        - `probs = F.softmax(logits, dim=-1)` : we apply a softmax to convert the logits to probabilities\n",
    "        - `index_next = torch.multinomial(probs, num_samples=1)` : we sample the next token index from the probability distribution\n",
    "        - `index = torch.cat((index, index_next), dim=1)` we concatanate the newly generated tokens to the provided indices \n",
    "        - `return index` : we return the index with the newly generated tokens concatanated\n",
    "\n",
    "- #### v) Define the Head class \n",
    "    - This class is for a single instance of a head from the multi head attention\n",
    "    - we accept one parameter of `head_size` for initialising\n",
    "        - `self.key = nn.Linear(n_embd, head_size, bias=False)` : we create a linear layer to project the input embedding to key\n",
    "        - `self.query = nn.Linear(n_embd, head_size, bias=False)` : we create a linear layer to project the input embedding to query\n",
    "        - `self.value = nn.Linear(n_embd, head_size, bias=False)` : we create a linear layer to project the input embedding to value\n",
    "        - `self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))` : we create a lower traingular matrix for casual masking to prevent attention to future tokens (so can only see present and past , cannot cheat by looking at the future)\n",
    "        - `self.dropout = nn.Dropout(dropout)` : we create a droput layer for regularization\n",
    "            - the dropout layer randomly sets some of the inputs to 0 based on our hyper parameter `dropout`\n",
    "                - `dropout` is the probability of the dropout occuring\n",
    "    \n",
    "    - we define the forward pass function for the self attendion head, we accept the input `x` that is a tensor (the embedding):\n",
    "        - `B, T, C = x.shape` : we unpack the input tensor for usage later on\n",
    "        - `k = self.key(x)` : we project the input embedding to the key\n",
    "        - `q = self.query(x)` : we project the input embedding to the query\n",
    "        - `wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)` : we compute the attention scores by :\n",
    "            - we transpose k to shape (B, head_size, T) for batch matrix multiplication (shape becomes (B, T, T) ) \n",
    "            - we do a matrix multiplication of queries and keys to get the attention scores\n",
    "        - `wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))` : we apply a casual buffer to ensure the head can only look at the past \n",
    "        - `wei = F.softmax(wei, dim=-1)` : we do a softmax to get the attention weights \n",
    "        - `wei = self.dropout(wei)` : we apply the dropout layer to         - `self.dropout = nn.Dropout(dropout)` : we create a droput layer for regularization of the results \n",
    "        - `v = self.value(x)` : we project the input embeddings to values\n",
    "        - `out = wei @ v` : we do a matrix multiplication of our attenstion scores to the input values to ensure that the inout information was not lost\n",
    "            - `shape` : (B, T, head_size)\n",
    "        - `return out` : we return the result with attention scores and values\n",
    "\n",
    "- #### vi) we define multi head attention class \n",
    "    - we accept `num_heads` and `head_size` as parameters for the initialisation\n",
    "        - `self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])` : we create `num_heads` amount of heads with each head having a dimensionality of `head_size` and add it to a list\n",
    "            - for e.g : if `head_size` is 7 and `num_heads` is 8 then in :\n",
    "                - we do `Head(7)` and it to the module list and repeat it 7 more times (8 times in total)\n",
    "        - `self.proj = nn.Linear(head_size * num_heads, n_embd)` : we create a linear layer to project concatenated head outputs back to embedding dimension\n",
    "        - `self.dropout = nn.Dropout(dropout)` : we create a dropout layer for regularization\n",
    "\n",
    "    - we define the forward pass for the multi head attention class, we accept an input of `x` which is a tensor (the embedding) : \n",
    "        - `out = torch.cat([h(x) for h in self.heads], dim=-1)` : we iterate over the heads in our list of heads and and concatanate all our outputs\n",
    "            - `for h in self.head` we go over each head in our list and for each head :\n",
    "                - `h(x)` we pass the input tensor embedding x of shape (B, T, C) and perform the forward pass on the individual head\n",
    "            - `torch.cat(...) : ` we concatanate all the outputs from each individual head to one tensor \n",
    "                - the one tensor has the shape (B, T, head_size * num_heads)\n",
    "            - `out = self.dropout(self.proj(out))` : we project and apply dropout\n",
    "                - `self.proj(out)` : we project concatenated head outputs back to embedding dimension\n",
    "                - `self.dropout(...)` : we apply the dropout layer to regularize the results \n",
    "            - `return out` : we return the projected and regularized outputs\n",
    "\n",
    "- #### vii) we define the feed forward class \n",
    "    - we accept the input `n_embd` (embedding dimension) for the initialistion :\n",
    "        - `self.net = nn.Sequential(nn.Linear(n_embd, 4 * n_embd), nn.ReLU(), nn.Linear(4 * n_embd, n_embd), nn.Dropout(dropout),)` : we define a sequential network comprising linear layers and ReLU activation and dropout for regularization \n",
    "            - `nn.Linear(n_embd, 4 * n_embd)` : Expand embedding dimension\n",
    "            - `nn.ReLU()` : Apply ReLU activation\n",
    "            - `nn.Linear(4 * n_embd, n_embd)` : Project back to original embedding dimension\n",
    "            - ` nn.Dropout(dropout)` : apply dropout for normalization \n",
    "    - we define the forward pass for the feed forward :\n",
    "        -  `return self.net(x)` : we just apply our sequential netword and return the input\n",
    "\n",
    "- #### viii) we define the transformer block class \n",
    "    - the transformer block represents a block of the transformer with communicationa and computation \n",
    "    - we accept `n_embd` (embedding dimension) and `n_head` (number of heads) as parameters for initialisation \n",
    "        - `head_size = n_embd // n_head` : we find the head size based on embedding dimension and number of heads\n",
    "        - `self.sa = MultiHeadAttention(n_head, head_size)` : we create a self attention module that is just a multi head attention module from the class we defined\n",
    "        - `self.ffwd = FeedForward(n_embd)` : we create our feed forward network based on the class we defined above\n",
    "        - `self.ln1 = nn.LayerNorm(n_embd)` : Layer normalization for after attention\n",
    "        - `self.ln2 = nn.LayerNorm(n_embd)` : Layer normalization for after feedforward\n",
    "    - we define the forward pass for the transformer block \n",
    "        - `y = self.sa(x)` : we apply multi-head self-attention\n",
    "        - `x = self.ln1(x + y)` : we add residual connection and apply layer normalization after the self attention\n",
    "        - `y = self.ffwd(x)` : we apply the feedforward network\n",
    "        - `x = self.ln2(x + y)` : we add residual connection and apply layer normalization after the feed forward\n",
    "        - `return x` : we return the tensor after performing the operations\n",
    "        \n",
    "- #### ix) initialise the model \n",
    "    - `model = GPTLanguageModel(vocab_size)` : we create a model using the `GPTLanguageModel` class we have defined\n",
    "    - `m = model.to(device)` : we move the model to our device (cpu or cuda or mps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a866531-a5ac-4ca8-9d79-e0bf75e14366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\"\n",
    "        Initializes the self-attention head.\n",
    "        \n",
    "        Args:\n",
    "            head_size (int): The dimensionality of each attention head.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Linear layers to project input embeddings to key, query, and value vectors\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        # Create a lower triangular matrix for causal masking to prevent attention to future tokens\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the self-attention head.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, T, C), where\n",
    "                              B = Batch size,\n",
    "                              T = Sequence length,\n",
    "                              C = Embedding dimension.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying self-attention, shape (B, T, head_size).\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape  # Unpack the input shape\n",
    "        \n",
    "        # Project input embeddings to keys and queries\n",
    "        k = self.key(x)    # Shape: (B, T, head_size)\n",
    "        q = self.query(x)  # Shape: (B, T, head_size)\n",
    "        \n",
    "        # Compute attention scores by taking the dot product of queries and keys\n",
    "        # Transpose k to shape (B, head_size, T) for batch matrix multiplication\n",
    "        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)  # Shape: (B, T, T)\n",
    "        \n",
    "        # Apply causal masking to ensure each position can only attend to previous positions\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Shape: (B, T, T)\n",
    "        \n",
    "        # Apply softmax to obtain attention weights\n",
    "        wei = F.softmax(wei, dim=-1)  # Shape: (B, T, T)\n",
    "        \n",
    "        # Apply dropout to the attention weights for regularization\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Project input embeddings to values\n",
    "        v = self.value(x)  # Shape: (B, T, head_size)\n",
    "        \n",
    "        # Perform weighted aggregation of the values based on attention weights\n",
    "        out = wei @ v  # Shape: (B, T, head_size)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        \"\"\"\n",
    "        Initializes the multi-head self-attention module.\n",
    "        \n",
    "        Args:\n",
    "            num_heads (int): Number of attention heads.\n",
    "            head_size (int): Dimensionality of each attention head.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Create a list of Head modules\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "        # Linear layer to project concatenated head outputs back to embedding dimension\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for multi-head self-attention.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, T, C).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after multi-head attention, shape (B, T, n_embd).\n",
    "        \"\"\"\n",
    "        # Concatenate outputs from all attention heads along the embedding dimension\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # Shape: (B, T, head_size * num_heads)\n",
    "        \n",
    "        # Project the concatenated outputs back to the original embedding dimension\n",
    "        out = self.dropout(self.proj(out))  # Shape: (B, T, n_embd)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple linear layer followed by a non-linearity.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        \"\"\"\n",
    "        Initializes the feedforward network.\n",
    "        \n",
    "        Args:\n",
    "            n_embd (int): Embedding dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Define a sequential network comprising linear layers and ReLU activation\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # Expand embedding dimension\n",
    "            nn.ReLU(),                      # Apply ReLU activation\n",
    "            nn.Linear(4 * n_embd, n_embd),  # Project back to original embedding dimension\n",
    "            nn.Dropout(dropout),            # Apply dropout for regularization\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the feedforward network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, T, C).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after feedforward processing, shape (B, T, C).\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer block.\n",
    "        \n",
    "        Args:\n",
    "            n_embd (int): Embedding dimension.\n",
    "            n_head (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head  # Determine head size based on embedding dimension and number of heads\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # Multi-head self-attention module\n",
    "        self.ffwd = FeedForward(n_embd)                  # Feedforward network\n",
    "        self.ln1 = nn.LayerNorm(n_embd)                  # Layer normalization after attention\n",
    "        self.ln2 = nn.LayerNorm(n_embd)                  # Layer normalization after feedforward\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the Transformer block.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, T, C).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after processing, shape (B, T, C).\n",
    "        \"\"\"\n",
    "        # Apply multi-head self-attention\n",
    "        y = self.sa(x)  # Shape: (B, T, C)\n",
    "        \n",
    "        # Add residual connection and apply layer normalization\n",
    "        x = self.ln1(x + y)  # Shape: (B, T, C)\n",
    "        \n",
    "        # Apply feedforward network\n",
    "        y = self.ffwd(x)  # Shape: (B, T, C)\n",
    "        \n",
    "        # Add residual connection and apply layer normalization\n",
    "        x = self.ln2(x + y)  # Shape: (B, T, C)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"GPT Language Model implementing Transformer architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        Initializes the GPT language model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary (number of unique tokens).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Token embedding table maps each token index to an embedding vector\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        \n",
    "        # Position embedding table provides positional information for each token in the sequence\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        # Stack multiple Transformer blocks to build the model's depth\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        \n",
    "        # Final layer normalization for stabilizing the output\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # Language modeling head projects the final embeddings to vocabulary logits\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        # Initialize weights using the defined method\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initializes weights of the model's layers.\n",
    "        \n",
    "        Args:\n",
    "            module (nn.Module): A module within the model.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Initialize linear layers with normal distribution (mean=0, std=0.02)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                # Initialize biases to zero\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # Initialize embedding layers with normal distribution (mean=0, std=0.02)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, index, targets=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the GPT language model.\n",
    "        \n",
    "        Args:\n",
    "            index (torch.Tensor): Input tensor of token indices, shape (B, T).\n",
    "            targets (torch.Tensor, optional): Target tensor of token indices, shape (B, T).\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "                - logits: Predicted logits for each token, shape (B, T, vocab_size).\n",
    "                - loss: Cross-entropy loss (if targets are provided), else None.\n",
    "        \"\"\"\n",
    "        B, T = index.shape  # Unpack batch size and sequence length\n",
    "        \n",
    "        # Retrieve token embeddings for the input indices\n",
    "        tok_emb = self.token_embedding_table(index)  # Shape: (B, T, C)\n",
    "        \n",
    "        # Create a range of positions and retrieve their embeddings\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # Shape: (T, C)\n",
    "        \n",
    "        # Add token and position embeddings to incorporate positional information\n",
    "        x = tok_emb + pos_emb  # Shape: (B, T, C)\n",
    "        \n",
    "        # Pass the embeddings through the stack of Transformer blocks\n",
    "        x = self.blocks(x)  # Shape: (B, T, C)\n",
    "        \n",
    "        # Apply final layer normalization\n",
    "        x = self.ln_f(x)  # Shape: (B, T, C)\n",
    "        \n",
    "        # Project the normalized embeddings to vocabulary logits\n",
    "        logits = self.lm_head(x)  # Shape: (B, T, vocab_size)\n",
    "        \n",
    "        # If targets are provided, compute the cross-entropy loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshape logits and targets for loss computation\n",
    "            logits = logits.view(B * T, -1)    # Shape: (B*T, vocab_size)\n",
    "            targets = targets.view(B * T)      # Shape: (B*T)\n",
    "            loss = F.cross_entropy(logits, targets)  # Scalar loss value\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generates new tokens based on the input context.\n",
    "        \n",
    "        Args:\n",
    "            index (torch.Tensor): Input tensor of token indices, shape (B, T).\n",
    "            max_new_tokens (int): Number of new tokens to generate.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Generated token indices, shape (B, T + max_new_tokens).\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop the input indices to the last block_size tokens to adhere to the model's maximum context length\n",
    "            index_cond = index[:, -block_size:]\n",
    "            \n",
    "            # Perform a forward pass to get the logits for the current context\n",
    "            logits, _ = self.forward(index_cond)\n",
    "            \n",
    "            # Focus on the logits of the last time step (the most recent token)\n",
    "            logits = logits[:, -1, :]  # Shape: (B, C)\n",
    "            \n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # Shape: (B, C)\n",
    "            \n",
    "            # Sample the next token index from the probability distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1)  # Shape: (B, 1)\n",
    "            \n",
    "            # Append the sampled token to the sequence\n",
    "            index = torch.cat((index, index_next), dim=1)  # Shape: (B, T + 1)\n",
    "        \n",
    "        return index\n",
    "\n",
    "# Instantiate the GPT language model with the specified vocabulary size\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "# Move the model to the specified device (CPU or GPU)\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08712a-8d3e-49fb-9b4f-5537a3df283b",
   "metadata": {},
   "source": [
    "## 7) create an AdamW optimiser and define the training loop\n",
    "\n",
    "- #### i) we create the optimizer we are gonna use \n",
    "    - we are using the adamW optimizer which is the adam oprimizer with added weight decay\n",
    "    - `optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)` : use `adamW` from `torch.optim` and we create the optimizer with our model parameters and our `learning_rate` hyper parameter\n",
    "\n",
    "- #### ii) define the training loop\n",
    "    - `for iter in range(max_iterations)` : we iterate for `max_iterations` times , `max_iterations` is our hyper parameter that says how many itterations we need to run the training loop for and in each iteration : \n",
    "        - ` if iter % evaluation_iterations == 0` : we check if we have hit our evaluation interval (i.e for e.g. every 100 iterations, it is defined by the `evaluation_iterations` hyper parameter) \n",
    "            - `losses = estimate_loss()` : we calculate our losses using the `estimate_loss` function we have defined above\n",
    "            - `print(f\"Iteration {iter}, training loss {losses['train']:.2f}, validation loss {losses['val']:.2f}\")` : we format and print the loss values for visualisation\n",
    "        - `inputs, targets = get_batch('train')` : we get our inputs and targets using the `get_batch` function that we have defined\n",
    "        - `logits, loss = model.forward(inputs, targets)` : we use our model's forward pass with targets since it is for training\n",
    "        - `optimizer.zero_grad(set_to_none=True)` : we reset the gradients of our tensors , we set it to None instead of 0's\n",
    "        - `loss.backward()` : we computes the gradient left by the current tensor wrt graph\n",
    "        - `optimizer.step()` : we update the model parameters after calculating the gradient\n",
    "    - `print(loss.item())` : we print the final loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d14b044-ac91-47ae-9d92-ed45a45ff004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss 4.38, validation loss 4.39\n",
      "Iteration 100, training loss 2.34, validation loss 2.40\n",
      "Iteration 200, training loss 1.97, validation loss 2.08\n",
      "Iteration 300, training loss 1.77, validation loss 1.89\n",
      "Iteration 400, training loss 1.63, validation loss 1.79\n",
      "Iteration 500, training loss 1.54, validation loss 1.73\n",
      "Iteration 600, training loss 1.47, validation loss 1.69\n",
      "Iteration 700, training loss 1.41, validation loss 1.66\n",
      "Iteration 800, training loss 1.35, validation loss 1.64\n",
      "Iteration 900, training loss 1.30, validation loss 1.65\n",
      "Iteration 1000, training loss 1.25, validation loss 1.65\n",
      "Iteration 1100, training loss 1.20, validation loss 1.65\n",
      "Iteration 1200, training loss 1.14, validation loss 1.64\n",
      "Iteration 1300, training loss 1.09, validation loss 1.66\n",
      "Iteration 1400, training loss 1.05, validation loss 1.67\n",
      "Iteration 1500, training loss 1.00, validation loss 1.69\n",
      "Iteration 1600, training loss 0.95, validation loss 1.72\n",
      "Iteration 1700, training loss 0.90, validation loss 1.73\n",
      "Iteration 1800, training loss 0.85, validation loss 1.77\n",
      "Iteration 1900, training loss 0.80, validation loss 1.81\n",
      "Iteration 2000, training loss 0.76, validation loss 1.83\n",
      "Iteration 2100, training loss 0.71, validation loss 1.88\n",
      "Iteration 2200, training loss 0.67, validation loss 1.90\n",
      "Iteration 2300, training loss 0.63, validation loss 1.94\n",
      "Iteration 2400, training loss 0.59, validation loss 2.00\n",
      "Iteration 2500, training loss 0.55, validation loss 2.02\n",
      "Iteration 2600, training loss 0.52, validation loss 2.07\n",
      "Iteration 2700, training loss 0.49, validation loss 2.12\n",
      "Iteration 2800, training loss 0.46, validation loss 2.16\n",
      "Iteration 2900, training loss 0.43, validation loss 2.19\n",
      "Iteration 3000, training loss 0.40, validation loss 2.23\n",
      "Iteration 3100, training loss 0.38, validation loss 2.26\n",
      "Iteration 3200, training loss 0.36, validation loss 2.31\n",
      "Iteration 3300, training loss 0.34, validation loss 2.36\n",
      "Iteration 3400, training loss 0.32, validation loss 2.40\n",
      "Iteration 3500, training loss 0.31, validation loss 2.46\n",
      "Iteration 3600, training loss 0.30, validation loss 2.45\n",
      "Iteration 3700, training loss 0.28, validation loss 2.52\n",
      "Iteration 3800, training loss 0.28, validation loss 2.57\n",
      "Iteration 3900, training loss 0.26, validation loss 2.60\n",
      "Iteration 4000, training loss 0.26, validation loss 2.63\n",
      "Iteration 4100, training loss 0.25, validation loss 2.68\n",
      "Iteration 4200, training loss 0.25, validation loss 2.67\n",
      "Iteration 4300, training loss 0.24, validation loss 2.72\n",
      "Iteration 4400, training loss 0.23, validation loss 2.77\n",
      "Iteration 4500, training loss 0.23, validation loss 2.81\n",
      "Iteration 4600, training loss 0.23, validation loss 2.84\n",
      "Iteration 4700, training loss 0.22, validation loss 2.86\n",
      "Iteration 4800, training loss 0.22, validation loss 2.89\n",
      "Iteration 4900, training loss 0.22, validation loss 2.90\n",
      "0.4126903712749481\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iterations):\n",
    "    if iter % evaluation_iterations == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Iteration {iter}, training loss {losses['train']:.2f}, validation loss {losses['val']:.2f}\")\n",
    "    inputs, targets = get_batch('train')\n",
    "    logits, loss = model.forward(inputs, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843c56be",
   "metadata": {},
   "source": [
    "## just some testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "849cb7d9-501d-4279-951e-0ea316e50c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi i am yung tings bers in cold make him list,\n",
      "Enor kinsman is old, what passes’d it shall.\n",
      "In tell you go to thee ni\n"
     ]
    }
   ],
   "source": [
    "test_prompt = 'hi i am yung ting'\n",
    "context = torch.tensor(encoder(test_prompt), dtype=torch.long, device=device)\n",
    "# context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_tokens = m.generate(context.unsqueeze(0), max_new_tokens=100)\n",
    "generated_chars = decoder(generated_tokens[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4df86-d99f-4cc6-b6e0-340160fdbcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49314257-8187-4d8d-bf6d-a2de7590e843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yungting-gpt",
   "language": "python",
   "name": "yugpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
